{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "\n",
    "Name: 黃有璿\n",
    "\n",
    "Student ID: 109062505\n",
    "\n",
    "GitHub ID: anitahuang121\n",
    "\n",
    "Kaggle name: Anita210503\n",
    "\n",
    "Kaggle private scoreboard snapshot: \n",
    "\n",
    "<img src=\"pics/KAGGLE.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T05:30:47.207362Z",
     "start_time": "2020-12-10T05:30:32.347301Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "emotion_data = pd.read_csv(\"emotion.csv\" , sep = \",\" , names = [\"tweet_id\" , \"emotion\"] , skiprows=[0])\n",
    "identification_data = pd.read_csv(\"data_identification.csv\" , sep = \",\" , names = [\"tweet_id\" , \"identification\"], skiprows=[0])\n",
    "tweets_data = pd.read_json(\"tweets_DM.json\" , lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T05:30:47.291143Z",
     "start_time": "2020-12-10T05:30:47.208684Z"
    }
   },
   "outputs": [],
   "source": [
    "identification_data.loc[identification_data['identification'] == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T05:30:47.299134Z",
     "start_time": "2020-12-10T05:30:47.292130Z"
    }
   },
   "outputs": [],
   "source": [
    "tweets_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T05:31:13.828340Z",
     "start_time": "2020-12-10T05:30:47.300021Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "data_df = json_normalize(tweets_data._source)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T05:31:14.313455Z",
     "start_time": "2020-12-10T05:31:13.829657Z"
    }
   },
   "outputs": [],
   "source": [
    "data_df = data_df.reindex(columns=['tweet.tweet_id','tweet.hashtags','tweet.text'])\n",
    "data_df = data_df.rename(columns={'tweet.tweet_id':'tweet_id' , 'tweet.hashtags':'hashtags' , 'tweet.text':'text' })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T05:31:14.329218Z",
     "start_time": "2020-12-10T05:31:14.314555Z"
    }
   },
   "outputs": [],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T05:31:16.017652Z",
     "start_time": "2020-12-10T05:31:14.330186Z"
    }
   },
   "outputs": [],
   "source": [
    "data_merge_emotion = pd.merge(data_df, emotion_data , on=['tweet_id'], how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T05:31:17.846772Z",
     "start_time": "2020-12-10T05:31:16.018757Z"
    }
   },
   "outputs": [],
   "source": [
    "data_all_info = pd.merge(data_merge_emotion, identification_data , on=['tweet_id'], how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T05:31:17.857785Z",
     "start_time": "2020-12-10T05:31:17.847913Z"
    }
   },
   "outputs": [],
   "source": [
    "data_all_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T05:31:17.972064Z",
     "start_time": "2020-12-10T05:31:17.858821Z"
    }
   },
   "outputs": [],
   "source": [
    "[(data_all_info['identification'] == 'train') & (data_all_info['emotion'].isnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T05:31:18.426699Z",
     "start_time": "2020-12-10T05:31:17.973132Z"
    }
   },
   "outputs": [],
   "source": [
    "data_all_info.loc[data_all_info['identification'] == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T05:31:18.636226Z",
     "start_time": "2020-12-10T05:31:18.427933Z"
    }
   },
   "outputs": [],
   "source": [
    "grouped = data_all_info.groupby(data_all_info.identification) \n",
    "train_df = grouped.get_group(\"train\") \n",
    "test_df = grouped.get_group(\"test\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T05:31:18.646598Z",
     "start_time": "2020-12-10T05:31:18.637625Z"
    }
   },
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T05:31:18.662317Z",
     "start_time": "2020-12-10T05:31:18.647438Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T05:31:21.420782Z",
     "start_time": "2020-12-10T05:31:18.663311Z"
    }
   },
   "outputs": [],
   "source": [
    "## save to pickle file\n",
    "\n",
    "train_df.to_pickle(\"train_df.pkl\") \n",
    "test_df.to_pickle(\"test_df.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T07:41:52.712836Z",
     "start_time": "2020-12-10T07:41:51.238836Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.callbacks import CSVLogger\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import RegexpTokenizer\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T07:41:52.716156Z",
     "start_time": "2020-12-10T07:41:52.714118Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import RegexpTokenizer\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T05:31:25.507622Z",
     "start_time": "2020-12-10T05:31:22.638732Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## load a pickle file\n",
    "train_df = pd.read_pickle(\"train_df.pkl\")\n",
    "test_df = pd.read_pickle(\"test_df.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T07:54:59.623111Z",
     "start_time": "2020-12-10T07:54:59.617250Z"
    }
   },
   "source": [
    "\n",
    "remove punctuation, <LH>, numbers, stopwords\n",
    "    \n",
    "    \n",
    "stemming and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T05:31:36.083214Z",
     "start_time": "2020-12-10T05:31:25.508931Z"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "def remove_punctuation(text):\n",
    "    no_punct = \"\".join([c for c in text if c not in string.punctuation])\n",
    "    return no_punct\n",
    "\n",
    "train_df['text_without_punct'] = train_df['text'].apply(lambda x : remove_punctuation(x))\n",
    "test_df['text_without_punct'] = test_df['text'].apply(lambda x : remove_punctuation(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T05:31:52.578423Z",
     "start_time": "2020-12-10T05:31:36.084518Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "filt = '[!\"#$%&()*,-./:;<=>?@[\\]^_`{|}~ ]'\n",
    "\n",
    "train_df['split'] = train_df['text'] .apply(lambda x: x.replace(\"<LH>\",\"\"))\n",
    "train_df['split'] = train_df['split'].apply(lambda x: re.sub(filt, ' ', x.lower()))\n",
    "train_df['split'] = train_df['split'].apply(lambda x: re.sub(r'\\d+', \" \", x))\n",
    "train_df['split'] = train_df['split'].apply(lambda x: re.sub('<[^<]+?>', \" \", x))\n",
    "train_df['split'] = train_df['split'].apply(lambda x: x.split(\" \"))\n",
    "\n",
    "test_df['split'] = test_df['text'] .apply(lambda x: x.replace(\"<LH>\",\"\"))\n",
    "test_df['split'] = test_df['split'].apply(lambda x: re.sub(filt, ' ', x.lower()))\n",
    "test_df['split'] = test_df['split'].apply(lambda x: re.sub(r'\\d+', \" \", x))\n",
    "test_df['split'] = test_df['split'].apply(lambda x: re.sub('<[^<]+?>', \" \", x))\n",
    "test_df['split'] = test_df['split'].apply(lambda x: x.split(\" \"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T05:32:00.217570Z",
     "start_time": "2020-12-10T05:31:52.579767Z"
    }
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "train_df['no_stopwords'] = train_df['split'].apply(lambda x: [item for item in x if item not in stop_words])\n",
    "train_df['no_stopwords'] = train_df['no_stopwords'].apply(lambda x: [item for item in x if len(item) > 1])\n",
    "test_df['no_stopwords'] = test_df['split'].apply(lambda x: [item for item in x if item not in stop_words])\n",
    "test_df['no_stopwords'] = test_df['no_stopwords'].apply(lambda x: [item for item in x if len(item) > 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T05:36:19.835455Z",
     "start_time": "2020-12-10T05:32:00.218710Z"
    }
   },
   "outputs": [],
   "source": [
    "porter = nltk.stem.PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "train_df['stemmed'] = train_df['no_stopwords'].apply(lambda x: [porter.stem(y) for y in x])\n",
    "test_df['stemmed'] = test_df['no_stopwords'].apply(lambda x: [porter.stem(y) for y in x])\n",
    "train_df['lemmatization'] = train_df['no_stopwords'].apply(lambda x: [lemmatizer.lemmatize(y) for y in x])\n",
    "test_df['lemmatization'] = test_df['no_stopwords'].apply(lambda x: [lemmatizer.lemmatize(y) for y in x])\n",
    "train_df['lem_ste'] = train_df['lemmatization'].apply(lambda x: [lemmatizer.lemmatize(y) for y in x])\n",
    "test_df['lem_ste'] = test_df['lemmatization'].apply(lambda x: [lemmatizer.lemmatize(y) for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T05:36:21.453509Z",
     "start_time": "2020-12-10T05:36:19.836611Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df['clean'] = train_df['lem_ste'].apply(lambda x: [item for item in x if not item.isnumeric() ])\n",
    "test_df['clean'] = test_df['lem_ste'].apply(lambda x: [item for item in x if not item.isnumeric() ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T05:36:22.029348Z",
     "start_time": "2020-12-10T05:36:21.454695Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df['cleaned'] = train_df['clean'].apply(' '.join)\n",
    "test_df['cleaned'] = test_df['clean'].apply(' '.join)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T07:46:05.667452Z",
     "start_time": "2020-12-10T07:45:31.953764Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=15000, \n",
    "                             use_idf=True,  \n",
    "                             tokenizer=tweet_tokenizer.tokenize,\n",
    "                             stop_words=stopwords.words('english').append('lh')\n",
    "                             )\n",
    "\n",
    "tfidf_vectorizer.fit(train_df['cleaned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T05:36:55.426718Z",
     "start_time": "2020-12-10T05:36:55.412012Z"
    }
   },
   "outputs": [],
   "source": [
    "print(tfidf_vectorizer.get_feature_names()[:100])\n",
    "print(tfidf_vectorizer.get_feature_names()[-100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T05:37:18.238681Z",
     "start_time": "2020-12-10T05:36:55.427672Z"
    }
   },
   "outputs": [],
   "source": [
    "#save cleaned data\n",
    "train_df.to_pickle(\"train_cleaned_df.pkl\") \n",
    "test_df.to_pickle(\"test_cleaned_df.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# METHOD1： DECISION TREE\n",
    "the result is not good, the best accuracy = 0.32959"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T05:39:04.394199Z",
     "start_time": "2020-12-10T05:37:18.244037Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# build analyzers (bag-of-words)\n",
    "BOW_500 = CountVectorizer(max_features=500, tokenizer=nltk.word_tokenize) \n",
    "\n",
    "# apply analyzer to training data\n",
    "tfidf_vectorizer.fit(train_df['text'])\n",
    "\n",
    "train_data_TFIDF_features_10000 = tfidf_vectorizer.transform(train_df['text'])\n",
    "\n",
    "## check dimension\n",
    "train_data_TFIDF_features_10000.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T05:40:14.080485Z",
     "start_time": "2020-12-10T05:39:04.395321Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# for a classificaiton problem, you need to provide both training & testing data\n",
    "X_train = tfidf_vectorizer.transform(train_df['text'])\n",
    "y_train = train_df['emotion']\n",
    "\n",
    "X_test = tfidf_vectorizer.transform(test_df['text'])\n",
    "y_test = test_df['emotion']\n",
    "\n",
    "## take a look at data dimension is a good habbit  :)\n",
    "print('X_train.shape: ', X_train.shape)\n",
    "print('y_train.shape: ', y_train.shape)\n",
    "print('X_test.shape: ', X_test.shape)\n",
    "print('y_test.shape: ', y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T07:10:23.909123Z",
     "start_time": "2020-12-10T05:40:14.081523Z"
    }
   },
   "outputs": [],
   "source": [
    "## build DecisionTree model\n",
    "DT_model = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "## training!\n",
    "DT_model = DT_model.fit(X_train, y_train)\n",
    "\n",
    "## predict!\n",
    "y_train_pred = DT_model.predict(X_train)\n",
    "y_test_pred = DT_model.predict(X_test)\n",
    "\n",
    "## so we get the pred result\n",
    "y_test_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T07:10:23.909531Z",
     "start_time": "2020-12-10T05:30:32.962Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_form = pd.read_pickle(\"test_df.pkl\")\n",
    "pred_form = pred_form.drop('hashtags', axis=1)\n",
    "pred_form = pred_form.drop('text', axis=1)\n",
    "pred_form = pred_form.drop('identification', axis=1)\n",
    "pred_form.rename(columns={'tweet_id':'id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T07:10:23.910040Z",
     "start_time": "2020-12-10T05:30:32.965Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_form['emotion'] = y_test_pred\n",
    "pred_form.to_csv('prediction.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# METHOD2： DEEP LEARNING\n",
    "This model has accuracy 0.42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data (X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T07:42:28.635718Z",
     "start_time": "2020-12-10T07:42:05.597908Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_pickle(\"train_cleaned_df.pkl\")\n",
    "test_df = pd.read_pickle(\"test_cleaned_df.pkl\")\n",
    "train_df = train_df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T07:42:28.641935Z",
     "start_time": "2020-12-10T07:42:28.637011Z"
    }
   },
   "outputs": [],
   "source": [
    "sep = int(train_df.shape[0] * 0.8) # 20% for validation\n",
    "\n",
    "train_dataset_X = train_df[:sep]\n",
    "train_dataset_y = train_df.emotion[:sep]\n",
    "validation_dataset_X = train_df[sep:]\n",
    "validation_dataset_y = train_df.emotion[sep:]\n",
    "\n",
    "all_trainset_X = train_df\n",
    "all_trainset_y = train_df.emotion\n",
    "\n",
    "print(train_dataset_X.shape)\n",
    "print(train_dataset_y.shape)\n",
    "print(validation_dataset_X.shape)\n",
    "print(validation_dataset_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T07:47:11.129612Z",
     "start_time": "2020-12-10T07:46:05.668661Z"
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "# standardize name (X, y) \n",
    "X_train = tfidf_vectorizer.transform(train_dataset_X['cleaned'])\n",
    "y_train = train_dataset_y\n",
    "\n",
    "X_val = tfidf_vectorizer.transform(validation_dataset_X['cleaned'])\n",
    "y_val = validation_dataset_y\n",
    "\n",
    "all_X_train =  tfidf_vectorizer.transform(all_trainset_X['cleaned'])\n",
    "all_y_train = all_trainset_y\n",
    "\n",
    "## check dimension is a good habbit \n",
    "print('X_train.shape: ', X_train.shape)\n",
    "print('y_train.shape: ', y_train.shape)\n",
    "print('X_val.shape: ', X_val.shape)\n",
    "print('y_val.shape: ', y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T07:47:21.449130Z",
     "start_time": "2020-12-10T07:47:11.130584Z"
    }
   },
   "outputs": [],
   "source": [
    "test_dataset_X = tfidf_vectorizer.transform(test_df['cleaned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T07:47:21.452190Z",
     "start_time": "2020-12-10T07:47:21.450355Z"
    }
   },
   "outputs": [],
   "source": [
    "print('x_test_data.shape: ', test_dataset_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deal with categorical label (y)\n",
    "\n",
    "Rather than put your label `train_df['emotion']` directly into a model, we have to process these categorical (or say nominal) label by ourselves. \n",
    "\n",
    "Here, we use the basic method [one-hot encoding](https://en.wikipedia.org/wiki/One-hot) to transform our categorical  labels to numerical ones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T07:47:21.488447Z",
     "start_time": "2020-12-10T07:47:21.453245Z"
    }
   },
   "outputs": [],
   "source": [
    "## deal with label (string -> one-hot)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "print('check label: ', label_encoder.classes_)\n",
    "print('\\n## Before convert')\n",
    "print('y_train[0:7]:\\n', y_train[0:7])\n",
    "print('\\ny_train.shape: ', y_train.shape)\n",
    "print('y_val.shape: ', y_val.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T07:47:21.785508Z",
     "start_time": "2020-12-10T07:47:21.489353Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def label_encode(le, labels):\n",
    "    enc = le.transform(labels)\n",
    "    return keras.utils.to_categorical(enc)\n",
    "\n",
    "def label_decode(le, one_hot_label):\n",
    "    dec = np.argmax(one_hot_label, axis=1)\n",
    "    return le.inverse_transform(dec)\n",
    "\n",
    "y_train = label_encode(label_encoder, y_train)\n",
    "y_val = label_encode(label_encoder, y_val)\n",
    "all_y_train = label_encode(label_encoder, all_y_train)\n",
    "\n",
    "print('\\n\\n## After convert')\n",
    "print('y_train[0:7]:\\n', y_train[0:7])\n",
    "print('\\ny_train.shape: ', y_train.shape)\n",
    "print('y_val.shape: ', y_val.shape)\n",
    "print('all_y_train.shape: ', all_y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BUILD MODEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T07:47:21.788630Z",
     "start_time": "2020-12-10T07:47:21.786538Z"
    }
   },
   "outputs": [],
   "source": [
    "# I/O check\n",
    "input_shape = X_train.shape[1]\n",
    "print('input_shape: ', input_shape)\n",
    "\n",
    "output_shape = len(label_encoder.classes_)\n",
    "print('output_shape: ', output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T07:48:25.800217Z",
     "start_time": "2020-12-10T07:48:25.461266Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, ReLU, Softmax, Dense\n",
    "\n",
    "# input layer\n",
    "model_input = Input(shape=(input_shape, ))  # 10000\n",
    "X = model_input\n",
    "\n",
    "# 1st hidden layer\n",
    "X_W1 = Dense(units=256)(X)  # 512\n",
    "H1 = ReLU()(X_W1)\n",
    "\n",
    "# 1st hidden layer\n",
    "H1_W2 = Dense(units=64)(H1)  # 256\n",
    "H2 = ReLU()(H1_W2)\n",
    "\n",
    "# 2nd hidden layer\n",
    "H2_W3 = Dense(units=32)(H2)  # 64\n",
    "H3 = ReLU()(H2_W3)\n",
    "\n",
    "\n",
    "# 2nd hidden layer\n",
    "H3_W4 = Dense(units=8)(H3)  # 32\n",
    "H4 = Softmax()(H3_W4)\n",
    "\n",
    "# output layer\n",
    "#H4_W5 = Dense(units=output_shape)(H4)  # 8\n",
    "#H5 = Softmax()(H4_W5)\n",
    "\n",
    "model_output = H4\n",
    "\n",
    "# create model\n",
    "model = Model(inputs=[model_input], outputs=[model_output])\n",
    "\n",
    "# loss function & optimizer\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# show model construction\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T07:51:57.326068Z",
     "start_time": "2020-12-10T07:51:57.076880Z"
    }
   },
   "outputs": [],
   "source": [
    "csv_logger = CSVLogger('logs/training_log_kaggle.csv')\n",
    "\n",
    "# training setting\n",
    "epochs = 5\n",
    "batch_size = 1000\n",
    "\n",
    "# training!\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size, \n",
    "                    callbacks=[csv_logger],\n",
    "                    validation_data = (X_val, y_val)\n",
    "                    )\n",
    "print('training finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T07:47:21.812803Z",
     "start_time": "2020-12-10T07:46:41.544Z"
    }
   },
   "outputs": [],
   "source": [
    "## predict\n",
    "pred_result_val = model.predict(X_val, batch_size=1000)\n",
    "pred_result_val[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T07:47:21.813337Z",
     "start_time": "2020-12-10T07:46:41.933Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_result_val = label_decode(label_encoder, pred_result_val)\n",
    "pred_result_val[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T07:47:21.813879Z",
     "start_time": "2020-12-10T07:46:42.209Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('testing accuracy: {}'.format(round(accuracy_score(label_decode(label_encoder, y_val), pred_result_val), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T07:47:21.814369Z",
     "start_time": "2020-12-10T07:46:42.586Z"
    }
   },
   "outputs": [],
   "source": [
    "#Let's take a look at the training log\n",
    "training_log = pd.DataFrame()\n",
    "training_log = pd.read_csv(\"logs/training_log.csv\")\n",
    "training_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T07:47:21.814981Z",
     "start_time": "2020-12-10T07:46:42.937Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Train accuracy', 'Val accuracy'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Train loss', 'Val loss'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T07:47:21.815572Z",
     "start_time": "2020-12-10T07:46:43.249Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_form = pd.read_pickle(\"test_df.pkl\")\n",
    "pred_form = pred_form.drop('hashtags', axis=1)\n",
    "pred_form = pred_form.drop('text', axis=1)\n",
    "pred_form = pred_form.drop('identification', axis=1)\n",
    "pred_form.rename(columns={'tweet_id':'id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T07:47:21.816467Z",
     "start_time": "2020-12-10T07:46:43.569Z"
    }
   },
   "outputs": [],
   "source": [
    "y_test_dense = model.predict(test_dataset_X)\n",
    "y_test_dense = label_decode(label_encoder, y_test_dense)\n",
    "pred_form['emotion'] = y_test_dense\n",
    "pred_form.to_csv('prediction_dense.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# METHOD3：Using LSTM\n",
    "accuracy = 0.46"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EMBEDDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T07:35:23.753843Z",
     "start_time": "2020-12-10T07:35:23.740894Z"
    }
   },
   "outputs": [],
   "source": [
    "Dictionary = {}\n",
    "for idx, token in enumerate(tfidf_vectorizer.get_feature_names()):\n",
    "    Dictionary[token] = idx\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T07:37:03.918214Z",
     "start_time": "2020-12-10T07:35:24.331626Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model_path = \"GoogleNews/GoogleNews-vectors-negative300.bin.gz\"\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(model_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T07:37:03.921801Z",
     "start_time": "2020-12-10T07:37:03.919370Z"
    }
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "def word2vec_matrix():\n",
    "    matrix_emb = np.zeros((len(Dictionary)+1, EMBEDDING_DIM)) # replace MAX_WORD to len(Dictionary)\n",
    "    for word, idx in Dictionary.items():\n",
    "        try:\n",
    "            vector =  word2vec_model.wv[word]\n",
    "        except:\n",
    "            vector = np.zeros(300,)\n",
    "        if idx < 15000:\n",
    "            matrix_emb[idx] = np.array(vector)\n",
    "    matrix_emb[15000] = np.zeros(300,)     \n",
    "    return matrix_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T07:37:03.931874Z",
     "start_time": "2020-12-10T07:37:03.922703Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 50\n",
    "\n",
    "def text_to_sequence(texts):\n",
    "    tokenized_texts = []\n",
    "    for string in texts:\n",
    "        tokenized_text = []\n",
    "        for idx, word in enumerate(string.split(' ')):\n",
    "            \n",
    "            # truncate\n",
    "            if idx >= MAX_SEQUENCE_LENGTH: break \n",
    "                \n",
    "            try:\n",
    "                token = token_dict[word.lower()]\n",
    "            except:\n",
    "                token = 10000\n",
    "            tokenized_text.append(token)\n",
    "            \n",
    "        # padding \n",
    "        if len(tokenized_text) < MAX_SEQUENCE_LENGTH: \n",
    "            tokenized_text.extend([10000]*(MAX_SEQUENCE_LENGTH-len(tokenized_text)))\n",
    "                \n",
    "        tokenized_texts.append(tokenized_text)\n",
    "\n",
    "    return np.array(tokenized_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T07:59:10.840967Z",
     "start_time": "2020-12-10T07:59:10.812700Z"
    }
   },
   "source": [
    "### emotion to one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T07:37:58.853502Z",
     "start_time": "2020-12-10T07:37:58.846070Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "emotion_array = ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'sadness', 'surprise','trust']\n",
    "def emotion_to_sequence(emotions):\n",
    "    tokenized_emotions = []\n",
    "    for string in emotions:\n",
    "        token = emotion_array.index(string)\n",
    "        tokenized_emotions.append(token)\n",
    "    return to_categorical(tokenized_emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T07:37:03.966236Z",
     "start_time": "2020-12-10T07:37:03.950208Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(emotion_array)\n",
    "\n",
    "print('check label: ', label_encoder.classes_)\n",
    "def label_encode(le, labels):\n",
    "    enc = le.transform(labels)\n",
    "    return keras.utils.to_categorical(enc)\n",
    "def label_decode(le, one_hot_label):\n",
    "    dec = np.argmax(one_hot_label, axis=1)\n",
    "    return le.inverse_transform(dec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T07:38:07.210319Z",
     "start_time": "2020-12-10T07:38:07.205872Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.initializers import Constant\n",
    "from keras.layers import Dense, LSTM, Dropout, Embedding, GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T07:37:06.093963Z",
     "start_time": "2020-12-10T07:37:03.984191Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = train_df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T07:37:18.161454Z",
     "start_time": "2020-12-10T07:37:06.095260Z"
    }
   },
   "outputs": [],
   "source": [
    "train_sequence = text_to_sequence(train_df['cleaned'])\n",
    "train_label = emotion_to_sequence(train_df['emotion'])\n",
    "\n",
    "test_sequence = text_to_sequence(test_df['cleaned'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T07:40:37.976104Z",
     "start_time": "2020-12-10T07:40:37.965929Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_WORD = 15000\n",
    "LSTM1_DIM = 256\n",
    "DENSE1_DIM = 512\n",
    "DENSE2_DIM = 64\n",
    "CATEGORY_NUM = 8\n",
    "BATCH_SIZE = 1000\n",
    "VALIDATION_SPLIT = 0.2\n",
    "EPOCHS = 6\n",
    "\n",
    "csv_logger = CSVLogger('logs/training_log_LSTM.csv')\n",
    "                    \n",
    "\n",
    "def model_setting(texts, labels, matrix_emb=\"uniform\"):    \n",
    "    DROPOUT = 0.2\n",
    "    \n",
    "    if matrix_emb != \"uniform\": matrix_emb = Constant(matrix_emb)\n",
    "    \n",
    "#     split data to train & validation\n",
    "    texts_train, texts_val, labels_train, labels_val = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # model build\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(MAX_WORD+1, EMBEDDING_DIM, embeddings_initializer=matrix_emb, input_length=MAX_SEQUENCE_LENGTH, trainable=True)) # with embedding matrix\n",
    "    model.add(LSTM(LSTM1_DIM, dropout=DROPOUT, recurrent_dropout=DROPOUT))\n",
    "    model.add(Dense(DENSE1_DIM, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(DENSE2_DIM, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(CATEGORY_NUM, kernel_initializer='normal', activation='softmax'))\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "    model.summary()\n",
    "\n",
    "    \n",
    "    history_callback = model.fit(\n",
    "        texts_train, \n",
    "        labels_train, \n",
    "        epochs=EPOCHS, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        callbacks=[csv_logger],\n",
    "        verbose=1, \n",
    "        shuffle=True, \n",
    "        validation_data=(texts_val, labels_val))\n",
    "    \n",
    "    pred_result = model.predict(test_sequence, batch_size=1000)\n",
    "    \n",
    "    pred_result_val = model.predict(texts_val, batch_size=1000)\n",
    "\n",
    "    return history_callback, pred_result, labels_val, pred_result_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T07:40:38.932544Z",
     "start_time": "2020-12-10T07:40:38.410463Z"
    }
   },
   "outputs": [],
   "source": [
    "history, pred_result, labels_val, pred_result_val  = model_setting(train_sequence, train_label, word2vec_matrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T07:40:38.933036Z",
     "start_time": "2020-12-10T07:40:39.305Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_result_val[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T07:40:39.014199Z",
     "start_time": "2020-12-10T07:40:39.002844Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_result_val = label_decode(label_encoder, pred_result_val)\n",
    "pred_result_val[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T07:40:40.037383Z",
     "start_time": "2020-12-10T07:40:40.020729Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('testing accuracy: {}'.format(round(accuracy_score(label_decode(label_encoder, labels_val), pred_result_val), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T07:40:40.616079Z",
     "start_time": "2020-12-10T07:40:40.599159Z"
    }
   },
   "outputs": [],
   "source": [
    "training_log = pd.DataFrame()\n",
    "training_log = pd.read_csv(\"logs/training_log_LSTM.csv\")\n",
    "training_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T07:37:18.495880Z",
     "start_time": "2020-12-10T07:35:31.903Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Train accuracy', 'Val accuracy'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Train loss', 'Val loss'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T07:37:18.496424Z",
     "start_time": "2020-12-10T07:35:32.304Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_form = pd.read_pickle(\"test_df.pkl\")\n",
    "pred_form = pred_form.drop('hashtags', axis=1)\n",
    "pred_form = pred_form.drop('text', axis=1)\n",
    "pred_form = pred_form.drop('identification', axis=1)\n",
    "pred_form.rename(columns={'tweet_id':'id'}, inplace=True)\n",
    "\n",
    "pred_form['emotion'] = label_decode(label_encoder, pred_result)\n",
    "pred_form.to_csv('prediction_lstm.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
