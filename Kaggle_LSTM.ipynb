{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "\n",
    "Name: 黃有璿\n",
    "\n",
    "Student ID: 109062505\n",
    "\n",
    "GitHub ID: anitahuang121\n",
    "\n",
    "Kaggle name: Anita210503\n",
    "\n",
    "Kaggle private scoreboard snapshot: \n",
    "\n",
    "<img src=\"pics/KAGGLE.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T08:58:30.634349Z",
     "start_time": "2020-12-10T08:58:30.339766Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, Embedding, GRU\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import RegexpTokenizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "emotion_data = pd.read_csv(\"emotion.csv\" , sep = \",\" , names = [\"tweet_id\" , \"emotion\"] , skiprows=[0])\n",
    "identification_data = pd.read_csv(\"data_identification.csv\" , sep = \",\" , names = [\"tweet_id\" , \"identification\"], skiprows=[0])\n",
    "tweets_data = pd.read_json(\"tweets_DM.json\" , lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identification_data.loc[identification_data['identification'] == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "data_df = json_normalize(tweets_data._source)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_df.reindex(columns=['tweet.tweet_id','tweet.hashtags','tweet.text'])\n",
    "data_df = data_df.rename(columns={'tweet.tweet_id':'tweet_id' , 'tweet.hashtags':'hashtags' , 'tweet.text':'text' })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_merge_emotion = pd.merge(data_df, emotion_data , on=['tweet_id'], how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all_info = pd.merge(data_merge_emotion, identification_data , on=['tweet_id'], how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(data_all_info['identification'] == 'train') & (data_all_info['emotion'].isnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = data_all_info.groupby(data_all_info.identification) \n",
    "train_df = grouped.get_group(\"train\") \n",
    "test_df = grouped.get_group(\"test\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save to pickle file\n",
    "\n",
    "train_df.to_pickle(\"train_df.pkl\") \n",
    "test_df.to_pickle(\"test_df.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.callbacks import CSVLogger\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import RegexpTokenizer\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## load a pickle file\n",
    "train_df = pd.read_pickle(\"train_df.pkl\")\n",
    "test_df = pd.read_pickle(\"test_df.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "remove punctuation, <LH>, numbers, stopwords\n",
    "    \n",
    "    \n",
    "stemming and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def remove_punctuation(text):\n",
    "    no_punct = \"\".join([c for c in text if c not in string.punctuation])\n",
    "    return no_punct\n",
    "\n",
    "train_df['text_without_punct'] = train_df['text'].apply(lambda x : remove_punctuation(x))\n",
    "test_df['text_without_punct'] = test_df['text'].apply(lambda x : remove_punctuation(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "filt = '[!\"#$%&()*,-./:;<=>?@[\\]^_`{|}~ ]'\n",
    "\n",
    "train_df['split'] = train_df['text'] .apply(lambda x: x.replace(\"<LH>\",\"\"))\n",
    "train_df['split'] = train_df['split'].apply(lambda x: re.sub(filt, ' ', x.lower()))\n",
    "train_df['split'] = train_df['split'].apply(lambda x: re.sub(r'\\d+', \" \", x))\n",
    "train_df['split'] = train_df['split'].apply(lambda x: re.sub('<[^<]+?>', \" \", x))\n",
    "train_df['split'] = train_df['split'].apply(lambda x: x.split(\" \"))\n",
    "\n",
    "test_df['split'] = test_df['text'] .apply(lambda x: x.replace(\"<LH>\",\"\"))\n",
    "test_df['split'] = test_df['split'].apply(lambda x: re.sub(filt, ' ', x.lower()))\n",
    "test_df['split'] = test_df['split'].apply(lambda x: re.sub(r'\\d+', \" \", x))\n",
    "test_df['split'] = test_df['split'].apply(lambda x: re.sub('<[^<]+?>', \" \", x))\n",
    "test_df['split'] = test_df['split'].apply(lambda x: x.split(\" \"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "train_df['no_stopwords'] = train_df['split'].apply(lambda x: [item for item in x if item not in stop_words])\n",
    "train_df['no_stopwords'] = train_df['no_stopwords'].apply(lambda x: [item for item in x if len(item) > 1])\n",
    "test_df['no_stopwords'] = test_df['split'].apply(lambda x: [item for item in x if item not in stop_words])\n",
    "test_df['no_stopwords'] = test_df['no_stopwords'].apply(lambda x: [item for item in x if len(item) > 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = nltk.stem.PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "train_df['stemmed'] = train_df['no_stopwords'].apply(lambda x: [porter.stem(y) for y in x])\n",
    "test_df['stemmed'] = test_df['no_stopwords'].apply(lambda x: [porter.stem(y) for y in x])\n",
    "train_df['lemmatization'] = train_df['no_stopwords'].apply(lambda x: [lemmatizer.lemmatize(y) for y in x])\n",
    "test_df['lemmatization'] = test_df['no_stopwords'].apply(lambda x: [lemmatizer.lemmatize(y) for y in x])\n",
    "train_df['lem_ste'] = train_df['lemmatization'].apply(lambda x: [lemmatizer.lemmatize(y) for y in x])\n",
    "test_df['lem_ste'] = test_df['lemmatization'].apply(lambda x: [lemmatizer.lemmatize(y) for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['clean'] = train_df['lem_ste'].apply(lambda x: [item for item in x if not item.isnumeric() ])\n",
    "test_df['clean'] = test_df['lem_ste'].apply(lambda x: [item for item in x if not item.isnumeric() ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['cleaned'] = train_df['clean'].apply(' '.join)\n",
    "test_df['cleaned'] = test_df['clean'].apply(' '.join)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=15000, \n",
    "                             use_idf=True,  \n",
    "                             tokenizer=tweet_tokenizer.tokenize,\n",
    "                             stop_words=stopwords.words('english').append('lh')\n",
    "                             )\n",
    "\n",
    "tfidf_vectorizer.fit(train_df['cleaned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tfidf_vectorizer.get_feature_names()[:100])\n",
    "print(tfidf_vectorizer.get_feature_names()[-100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save cleaned data\n",
    "train_df.to_pickle(\"train_cleaned_df.pkl\") \n",
    "test_df.to_pickle(\"test_cleaned_df.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T08:59:14.030567Z",
     "start_time": "2020-12-10T08:58:54.575587Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_pickle(\"train_cleaned_df.pkl\") \n",
    "test_df = pd.read_pickle(\"test_cleaned_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T06:06:23.487338Z",
     "start_time": "2020-12-09T06:06:23.457981Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "      <th>identification</th>\n",
       "      <th>text_without_punct</th>\n",
       "      <th>split</th>\n",
       "      <th>no_stopwords</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatization</th>\n",
       "      <th>lem_ste</th>\n",
       "      <th>clean</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Snapchat]</td>\n",
       "      <td>0x376b20</td>\n",
       "      <td>People who post \"add me on #Snapchat\" must be ...</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>train</td>\n",
       "      <td>People who post add me on Snapchat must be deh...</td>\n",
       "      <td>[people, who, post, , add, me, on, , snapchat,...</td>\n",
       "      <td>[people, post, add, snapchat, must, dehydrated...</td>\n",
       "      <td>[peopl, post, add, snapchat, must, dehydr, cuz...</td>\n",
       "      <td>[people, post, add, snapchat, must, dehydrated...</td>\n",
       "      <td>[people, post, add, snapchat, must, dehydrated...</td>\n",
       "      <td>[people, post, add, snapchat, must, dehydrated...</td>\n",
       "      <td>people post add snapchat must dehydrated cuz m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[freepress, TrumpLegacy, CNN]</td>\n",
       "      <td>0x2d5350</td>\n",
       "      <td>@brianklaas As we see, Trump is dangerous to #...</td>\n",
       "      <td>sadness</td>\n",
       "      <td>train</td>\n",
       "      <td>brianklaas As we see Trump is dangerous to fre...</td>\n",
       "      <td>[, brianklaas, as, we, see, , trump, is, dange...</td>\n",
       "      <td>[brianklaas, see, trump, dangerous, freepress,...</td>\n",
       "      <td>[brianklaa, see, trump, danger, freepress, aro...</td>\n",
       "      <td>[brianklaas, see, trump, dangerous, freepress,...</td>\n",
       "      <td>[brianklaas, see, trump, dangerous, freepress,...</td>\n",
       "      <td>[brianklaas, see, trump, dangerous, freepress,...</td>\n",
       "      <td>brianklaas see trump dangerous freepress aroun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x1cd5b0</td>\n",
       "      <td>Now ISSA is stalking Tasha 😂😂😂 &lt;LH&gt;</td>\n",
       "      <td>fear</td>\n",
       "      <td>train</td>\n",
       "      <td>Now ISSA is stalking Tasha 😂😂😂 LH</td>\n",
       "      <td>[now, issa, is, stalking, tasha, 😂😂😂, ]</td>\n",
       "      <td>[issa, stalking, tasha, 😂😂😂]</td>\n",
       "      <td>[issa, stalk, tasha, 😂😂😂]</td>\n",
       "      <td>[issa, stalking, tasha, 😂😂😂]</td>\n",
       "      <td>[issa, stalking, tasha, 😂😂😂]</td>\n",
       "      <td>[issa, stalking, tasha, 😂😂😂]</td>\n",
       "      <td>issa stalking tasha 😂😂😂</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[authentic, LaughOutLoud]</td>\n",
       "      <td>0x1d755c</td>\n",
       "      <td>@RISKshow @TheKevinAllison Thx for the BEST TI...</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "      <td>RISKshow TheKevinAllison Thx for the BEST TIME...</td>\n",
       "      <td>[, riskshow, , thekevinallison, thx, for, the,...</td>\n",
       "      <td>[riskshow, thekevinallison, thx, best, time, t...</td>\n",
       "      <td>[riskshow, thekevinallison, thx, best, time, t...</td>\n",
       "      <td>[riskshow, thekevinallison, thx, best, time, t...</td>\n",
       "      <td>[riskshow, thekevinallison, thx, best, time, t...</td>\n",
       "      <td>[riskshow, thekevinallison, thx, best, time, t...</td>\n",
       "      <td>riskshow thekevinallison thx best time tonight...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x2c91a8</td>\n",
       "      <td>Still waiting on those supplies Liscus. &lt;LH&gt;</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>train</td>\n",
       "      <td>Still waiting on those supplies Liscus LH</td>\n",
       "      <td>[still, waiting, on, those, supplies, liscus, , ]</td>\n",
       "      <td>[still, waiting, supplies, liscus]</td>\n",
       "      <td>[still, wait, suppli, liscu]</td>\n",
       "      <td>[still, waiting, supply, liscus]</td>\n",
       "      <td>[still, waiting, supply, liscus]</td>\n",
       "      <td>[still, waiting, supply, liscus]</td>\n",
       "      <td>still waiting supply liscus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867526</th>\n",
       "      <td>[NoWonder, Happy]</td>\n",
       "      <td>0x321566</td>\n",
       "      <td>I'm SO HAPPY!!! #NoWonder the name of this sho...</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "      <td>Im SO HAPPY NoWonder the name of this show Hap...</td>\n",
       "      <td>[i'm, so, happy, , , , , nowonder, the, name, ...</td>\n",
       "      <td>[i'm, happy, nowonder, name, show, happy, happ...</td>\n",
       "      <td>[i'm, happi, nowond, name, show, happi, happys...</td>\n",
       "      <td>[i'm, happy, nowonder, name, show, happy, happ...</td>\n",
       "      <td>[i'm, happy, nowonder, name, show, happy, happ...</td>\n",
       "      <td>[i'm, happy, nowonder, name, show, happy, happ...</td>\n",
       "      <td>i'm happy nowonder name show happy happysyfy s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867527</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x38959e</td>\n",
       "      <td>In every circumtance I'd like to be thankful t...</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "      <td>In every circumtance Id like to be thankful to...</td>\n",
       "      <td>[in, every, circumtance, i'd, like, to, be, th...</td>\n",
       "      <td>[every, circumtance, i'd, like, thankful, almi...</td>\n",
       "      <td>[everi, circumt, i'd, like, thank, almighti, j...</td>\n",
       "      <td>[every, circumtance, i'd, like, thankful, almi...</td>\n",
       "      <td>[every, circumtance, i'd, like, thankful, almi...</td>\n",
       "      <td>[every, circumtance, i'd, like, thankful, almi...</td>\n",
       "      <td>every circumtance i'd like thankful almighty j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867528</th>\n",
       "      <td>[blessyou]</td>\n",
       "      <td>0x2cbca6</td>\n",
       "      <td>there's currently two girls walking around the...</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "      <td>theres currently two girls walking around the ...</td>\n",
       "      <td>[there's, currently, two, girls, walking, arou...</td>\n",
       "      <td>[there's, currently, two, girls, walking, arou...</td>\n",
       "      <td>[there', current, two, girl, walk, around, lib...</td>\n",
       "      <td>[there's, currently, two, girl, walking, aroun...</td>\n",
       "      <td>[there's, currently, two, girl, walking, aroun...</td>\n",
       "      <td>[there's, currently, two, girl, walking, aroun...</td>\n",
       "      <td>there's currently two girl walking around libr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867533</th>\n",
       "      <td>[]</td>\n",
       "      <td>0x24faed</td>\n",
       "      <td>Ah, corporate life, where you can date &lt;LH&gt; us...</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "      <td>Ah corporate life where you can date LH using ...</td>\n",
       "      <td>[ah, , corporate, life, , where, you, can, dat...</td>\n",
       "      <td>[ah, corporate, life, date, using, relative, a...</td>\n",
       "      <td>[ah, corpor, life, date, use, rel, anachron, l...</td>\n",
       "      <td>[ah, corporate, life, date, using, relative, a...</td>\n",
       "      <td>[ah, corporate, life, date, using, relative, a...</td>\n",
       "      <td>[ah, corporate, life, date, using, relative, a...</td>\n",
       "      <td>ah corporate life date using relative anachron...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1867534</th>\n",
       "      <td>[Sundayvibes]</td>\n",
       "      <td>0x34be8c</td>\n",
       "      <td>Blessed to be living #Sundayvibes &lt;LH&gt;</td>\n",
       "      <td>joy</td>\n",
       "      <td>train</td>\n",
       "      <td>Blessed to be living Sundayvibes LH</td>\n",
       "      <td>[blessed, to, be, living, , sundayvibes, ]</td>\n",
       "      <td>[blessed, living, sundayvibes]</td>\n",
       "      <td>[bless, live, sundayvib]</td>\n",
       "      <td>[blessed, living, sundayvibes]</td>\n",
       "      <td>[blessed, living, sundayvibes]</td>\n",
       "      <td>[blessed, living, sundayvibes]</td>\n",
       "      <td>blessed living sundayvibes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1455563 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              hashtags  tweet_id  \\\n",
       "0                           [Snapchat]  0x376b20   \n",
       "1        [freepress, TrumpLegacy, CNN]  0x2d5350   \n",
       "3                                   []  0x1cd5b0   \n",
       "5            [authentic, LaughOutLoud]  0x1d755c   \n",
       "6                                   []  0x2c91a8   \n",
       "...                                ...       ...   \n",
       "1867526              [NoWonder, Happy]  0x321566   \n",
       "1867527                             []  0x38959e   \n",
       "1867528                     [blessyou]  0x2cbca6   \n",
       "1867533                             []  0x24faed   \n",
       "1867534                  [Sundayvibes]  0x34be8c   \n",
       "\n",
       "                                                      text       emotion  \\\n",
       "0        People who post \"add me on #Snapchat\" must be ...  anticipation   \n",
       "1        @brianklaas As we see, Trump is dangerous to #...       sadness   \n",
       "3                      Now ISSA is stalking Tasha 😂😂😂 <LH>          fear   \n",
       "5        @RISKshow @TheKevinAllison Thx for the BEST TI...           joy   \n",
       "6             Still waiting on those supplies Liscus. <LH>  anticipation   \n",
       "...                                                    ...           ...   \n",
       "1867526  I'm SO HAPPY!!! #NoWonder the name of this sho...           joy   \n",
       "1867527  In every circumtance I'd like to be thankful t...           joy   \n",
       "1867528  there's currently two girls walking around the...           joy   \n",
       "1867533  Ah, corporate life, where you can date <LH> us...           joy   \n",
       "1867534             Blessed to be living #Sundayvibes <LH>           joy   \n",
       "\n",
       "        identification                                 text_without_punct  \\\n",
       "0                train  People who post add me on Snapchat must be deh...   \n",
       "1                train  brianklaas As we see Trump is dangerous to fre...   \n",
       "3                train                  Now ISSA is stalking Tasha 😂😂😂 LH   \n",
       "5                train  RISKshow TheKevinAllison Thx for the BEST TIME...   \n",
       "6                train          Still waiting on those supplies Liscus LH   \n",
       "...                ...                                                ...   \n",
       "1867526          train  Im SO HAPPY NoWonder the name of this show Hap...   \n",
       "1867527          train  In every circumtance Id like to be thankful to...   \n",
       "1867528          train  theres currently two girls walking around the ...   \n",
       "1867533          train  Ah corporate life where you can date LH using ...   \n",
       "1867534          train                Blessed to be living Sundayvibes LH   \n",
       "\n",
       "                                                     split  \\\n",
       "0        [people, who, post, , add, me, on, , snapchat,...   \n",
       "1        [, brianklaas, as, we, see, , trump, is, dange...   \n",
       "3                  [now, issa, is, stalking, tasha, 😂😂😂, ]   \n",
       "5        [, riskshow, , thekevinallison, thx, for, the,...   \n",
       "6        [still, waiting, on, those, supplies, liscus, , ]   \n",
       "...                                                    ...   \n",
       "1867526  [i'm, so, happy, , , , , nowonder, the, name, ...   \n",
       "1867527  [in, every, circumtance, i'd, like, to, be, th...   \n",
       "1867528  [there's, currently, two, girls, walking, arou...   \n",
       "1867533  [ah, , corporate, life, , where, you, can, dat...   \n",
       "1867534         [blessed, to, be, living, , sundayvibes, ]   \n",
       "\n",
       "                                              no_stopwords  \\\n",
       "0        [people, post, add, snapchat, must, dehydrated...   \n",
       "1        [brianklaas, see, trump, dangerous, freepress,...   \n",
       "3                             [issa, stalking, tasha, 😂😂😂]   \n",
       "5        [riskshow, thekevinallison, thx, best, time, t...   \n",
       "6                       [still, waiting, supplies, liscus]   \n",
       "...                                                    ...   \n",
       "1867526  [i'm, happy, nowonder, name, show, happy, happ...   \n",
       "1867527  [every, circumtance, i'd, like, thankful, almi...   \n",
       "1867528  [there's, currently, two, girls, walking, arou...   \n",
       "1867533  [ah, corporate, life, date, using, relative, a...   \n",
       "1867534                     [blessed, living, sundayvibes]   \n",
       "\n",
       "                                                   stemmed  \\\n",
       "0        [peopl, post, add, snapchat, must, dehydr, cuz...   \n",
       "1        [brianklaa, see, trump, danger, freepress, aro...   \n",
       "3                                [issa, stalk, tasha, 😂😂😂]   \n",
       "5        [riskshow, thekevinallison, thx, best, time, t...   \n",
       "6                             [still, wait, suppli, liscu]   \n",
       "...                                                    ...   \n",
       "1867526  [i'm, happi, nowond, name, show, happi, happys...   \n",
       "1867527  [everi, circumt, i'd, like, thank, almighti, j...   \n",
       "1867528  [there', current, two, girl, walk, around, lib...   \n",
       "1867533  [ah, corpor, life, date, use, rel, anachron, l...   \n",
       "1867534                           [bless, live, sundayvib]   \n",
       "\n",
       "                                             lemmatization  \\\n",
       "0        [people, post, add, snapchat, must, dehydrated...   \n",
       "1        [brianklaas, see, trump, dangerous, freepress,...   \n",
       "3                             [issa, stalking, tasha, 😂😂😂]   \n",
       "5        [riskshow, thekevinallison, thx, best, time, t...   \n",
       "6                         [still, waiting, supply, liscus]   \n",
       "...                                                    ...   \n",
       "1867526  [i'm, happy, nowonder, name, show, happy, happ...   \n",
       "1867527  [every, circumtance, i'd, like, thankful, almi...   \n",
       "1867528  [there's, currently, two, girl, walking, aroun...   \n",
       "1867533  [ah, corporate, life, date, using, relative, a...   \n",
       "1867534                     [blessed, living, sundayvibes]   \n",
       "\n",
       "                                                   lem_ste  \\\n",
       "0        [people, post, add, snapchat, must, dehydrated...   \n",
       "1        [brianklaas, see, trump, dangerous, freepress,...   \n",
       "3                             [issa, stalking, tasha, 😂😂😂]   \n",
       "5        [riskshow, thekevinallison, thx, best, time, t...   \n",
       "6                         [still, waiting, supply, liscus]   \n",
       "...                                                    ...   \n",
       "1867526  [i'm, happy, nowonder, name, show, happy, happ...   \n",
       "1867527  [every, circumtance, i'd, like, thankful, almi...   \n",
       "1867528  [there's, currently, two, girl, walking, aroun...   \n",
       "1867533  [ah, corporate, life, date, using, relative, a...   \n",
       "1867534                     [blessed, living, sundayvibes]   \n",
       "\n",
       "                                                     clean  \\\n",
       "0        [people, post, add, snapchat, must, dehydrated...   \n",
       "1        [brianklaas, see, trump, dangerous, freepress,...   \n",
       "3                             [issa, stalking, tasha, 😂😂😂]   \n",
       "5        [riskshow, thekevinallison, thx, best, time, t...   \n",
       "6                         [still, waiting, supply, liscus]   \n",
       "...                                                    ...   \n",
       "1867526  [i'm, happy, nowonder, name, show, happy, happ...   \n",
       "1867527  [every, circumtance, i'd, like, thankful, almi...   \n",
       "1867528  [there's, currently, two, girl, walking, aroun...   \n",
       "1867533  [ah, corporate, life, date, using, relative, a...   \n",
       "1867534                     [blessed, living, sundayvibes]   \n",
       "\n",
       "                                                   cleaned  \n",
       "0        people post add snapchat must dehydrated cuz m...  \n",
       "1        brianklaas see trump dangerous freepress aroun...  \n",
       "3                                  issa stalking tasha 😂😂😂  \n",
       "5        riskshow thekevinallison thx best time tonight...  \n",
       "6                              still waiting supply liscus  \n",
       "...                                                    ...  \n",
       "1867526  i'm happy nowonder name show happy happysyfy s...  \n",
       "1867527  every circumtance i'd like thankful almighty j...  \n",
       "1867528  there's currently two girl walking around libr...  \n",
       "1867533  ah corporate life date using relative anachron...  \n",
       "1867534                         blessed living sundayvibes  \n",
       "\n",
       "[1455563 rows x 13 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T08:59:48.781420Z",
     "start_time": "2020-12-10T08:59:14.031856Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_features=15000,\n",
       "                stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
       "                            'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
       "                            \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
       "                            'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
       "                            'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
       "                            'itself', ...],\n",
       "                tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7f00021fa3c8>>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_tokenizer = TweetTokenizer()\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=15000, \n",
    "                             use_idf=True,  \n",
    "                             tokenizer=tweet_tokenizer.tokenize,\n",
    "                             stop_words=stopwords.words('english')\n",
    "                             )\n",
    "\n",
    "tfidf_vectorizer.fit(train_df['cleaned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T08:59:48.797566Z",
     "start_time": "2020-12-10T08:59:48.782729Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'\", '+', '\\\\', \"a's\", 'aa', 'aaa', 'aajtak', 'aamir', 'aap', 'aaron', 'aaroncarter', 'aaronrodgers', 'aarti', 'ab', 'abandon', 'abandoned', 'abba', 'abbey', 'abby', 'abc', 'abcnetwork', 'abcnews', 'abcpolitics', 'aberdeen', 'abhlatino', 'abi', 'abide', 'abiding', 'ability', 'able', 'aboard', 'abominable', 'abomination', 'abortion', 'abound', 'aboveonly', 'abraham', 'abroad', 'absence', 'absent', 'absolute', 'absolutely', 'absorbed', 'absurd', 'abt', 'abu', 'abundance', 'abundant', 'abundantly', 'abuse', 'abused', 'abuser', 'abusing', 'abusive', 'abyss', 'ac', 'aca', 'academic', 'academy', 'acc', 'accent', 'accept', 'acceptable', 'acceptance', 'accepted', 'accepting', 'accepts', 'access', 'accessible', 'accessory', 'accident', 'accidentally', 'accomplish', 'accomplished', 'accomplishing', 'accomplishment', 'accord', 'according', 'accordingly', 'account', 'accountability', 'accountable', 'accountant', 'accounting', 'acct', 'accuracy', 'accurate', 'accusation', 'accuse', 'accused', 'accusing', 'ace', 'ache', 'achieve', 'achieved', 'achievement', 'achievemints', 'achieving', 'aching', 'acid']\n",
      "['🙀', '🙁', '🙂', '🙃', '🙄', '🙅', '🙆', '🙇', '🙈', '🙉', '🙊', '🙋', '🙌', '🙍', '🙏', '🚀', '🚂', '🚑', '🚒', '🚔', '🚗', '🚘', '🚙', '🚨', '🚫', '🚬', '🚮', '🚴', '🚶', '🛌', '🛍', '🛐', '🛫', '🤐', '🤑', '🤒', '🤓', '🤔', '🤕', '🤖', '🤗', '🤘', '🤙', '🤚', '🤛', '🤜', '🤝', '🤞', '\\U0001f91f', '🤠', '🤡', '🤢', '🤣', '🤤', '🤥', '🤦', '🤧', '\\U0001f928', '\\U0001f929', '\\U0001f92a', '\\U0001f92b', '\\U0001f92c', '\\U0001f92d', '\\U0001f92e', '\\U0001f92f', '🤰', '🤳', '🤴', '🤵', '🤶', '🤷', '🤸', '🥀', '🥁', '🥂', '🥃', '🥇', '🥊', '🥑', '🥓', '🥕', '🥗', '🥞', '\\U0001f967', '🦁', '🦃', '🦄', '🦅', '🦇', '🦈', '🦉', '🦋', '🦌', '🧀', '\\U0001f9d0', '\\U0001f9e1', '\\U000e0062', '\\U000e0067', '\\U000e0073', '\\U000e007f']\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_vectorizer.get_feature_names()[:100])\n",
    "print(tfidf_vectorizer.get_feature_names()[-100:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deal with categorical label(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T08:59:48.807214Z",
     "start_time": "2020-12-10T08:59:48.798842Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "emotion_array = ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'sadness', 'surprise','trust']\n",
    "def emotion_to_sequence(emotions):\n",
    "    tokenized_emotions = []\n",
    "    for string in emotions:\n",
    "        token = emotion_array.index(string)\n",
    "        tokenized_emotions.append(token)\n",
    "    return to_categorical(tokenized_emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T08:59:48.817952Z",
     "start_time": "2020-12-10T08:59:48.808150Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check label:  ['anger' 'anticipation' 'disgust' 'fear' 'joy' 'sadness' 'surprise'\n",
      " 'trust']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(emotion_array)\n",
    "\n",
    "print('check label: ', label_encoder.classes_)\n",
    "def label_encode(le, labels):\n",
    "    enc = le.transform(labels)\n",
    "    return keras.utils.to_categorical(enc)\n",
    "def label_decode(le, one_hot_label):\n",
    "    dec = np.argmax(one_hot_label, axis=1)\n",
    "    return le.inverse_transform(dec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T08:59:48.842820Z",
     "start_time": "2020-12-10T08:59:48.818871Z"
    }
   },
   "outputs": [],
   "source": [
    "Dictionary = {}\n",
    "for idx, token in enumerate(tfidf_vectorizer.get_feature_names()):\n",
    "    Dictionary[token] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T09:01:37.763108Z",
     "start_time": "2020-12-10T08:59:48.843928Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model_path = \"GoogleNews/GoogleNews-vectors-negative300.bin.gz\"\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(model_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T09:01:37.805897Z",
     "start_time": "2020-12-10T09:01:37.781366Z"
    }
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "def word2vec_matrix():\n",
    "    matrix_emb = np.zeros((len(Dictionary)+1, EMBEDDING_DIM)) # replace MAX_WORD to len(Dictionary)\n",
    "    for word, idx in Dictionary.items():\n",
    "        try:\n",
    "            vector =  word2vec_model.wv[word]\n",
    "        except:\n",
    "            vector = np.zeros(300,)\n",
    "        if idx < 15000:\n",
    "            matrix_emb[idx] = np.array(vector)\n",
    "    matrix_emb[15000] = np.zeros(300,)     \n",
    "    return matrix_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T09:01:37.822625Z",
     "start_time": "2020-12-10T09:01:37.808746Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 50\n",
    "\n",
    "def text_to_sequence(texts):\n",
    "    tokenized_texts = []\n",
    "    for string in texts:\n",
    "        tokenized_text = []\n",
    "        for idx, word in enumerate(string.split(' ')):\n",
    "            \n",
    "            # truncate\n",
    "            if idx >= MAX_SEQUENCE_LENGTH: break \n",
    "                \n",
    "            try:\n",
    "                token = token_dict[word.lower()]\n",
    "            except:\n",
    "                token = 10000\n",
    "            tokenized_text.append(token)\n",
    "            \n",
    "        # padding \n",
    "        if len(tokenized_text) < MAX_SEQUENCE_LENGTH: \n",
    "            tokenized_text.extend([10000]*(MAX_SEQUENCE_LENGTH-len(tokenized_text)))\n",
    "                \n",
    "        tokenized_texts.append(tokenized_text)\n",
    "\n",
    "    return np.array(tokenized_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T09:01:54.839863Z",
     "start_time": "2020-12-10T09:01:37.823609Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = train_df.sample(frac=1)\n",
    "train_sequence = text_to_sequence(train_df['cleaned'])\n",
    "train_label = emotion_to_sequence(train_df['emotion'])\n",
    "\n",
    "test_sequence = text_to_sequence(test_df['cleaned'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-10T09:05:11.041453Z",
     "start_time": "2020-12-10T09:05:11.033868Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_WORD = 15000\n",
    "LSTM1_DIM = 256\n",
    "DENSE1_DIM = 512\n",
    "DENSE2_DIM = 64\n",
    "CATEGORY_NUM = 8\n",
    "BATCH_SIZE = 1000\n",
    "VALIDATION_SPLIT = 0.2\n",
    "EPOCHS = 6\n",
    "\n",
    "csv_logger = CSVLogger('logs/training_log_LSTM.csv')\n",
    "                    \n",
    "\n",
    "def model_setting(texts, labels, matrix_emb=\"uniform\"):    \n",
    "    DROPOUT = 0.2\n",
    "    \n",
    "    if matrix_emb != \"uniform\": matrix_emb = Constant(matrix_emb)\n",
    "    \n",
    "#     split data to train & validation\n",
    "    texts_train, texts_val, labels_train, labels_val = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # model build\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(MAX_WORD+1, EMBEDDING_DIM, embeddings_initializer=matrix_emb, input_length=MAX_SEQUENCE_LENGTH, trainable=True)) # with embedding matrix\n",
    "    model.add(LSTM(LSTM1_DIM, dropout=DROPOUT, recurrent_dropout=DROPOUT))\n",
    "    model.add(Dense(DENSE1_DIM, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(DENSE2_DIM, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(CATEGORY_NUM, kernel_initializer='normal', activation='softmax'))\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "    model.summary()\n",
    "\n",
    "    \n",
    "    history_callback = model.fit(\n",
    "        texts_train, \n",
    "        labels_train, \n",
    "        epochs=EPOCHS, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        callbacks=[csv_logger],\n",
    "        verbose=1, \n",
    "        shuffle=True, \n",
    "        validation_data=(texts_val, labels_val))\n",
    "    \n",
    "    pred_result = model.predict(test_sequence, batch_size=1000)\n",
    "    \n",
    "    pred_result_val = model.predict(texts_val, batch_size=1000)\n",
    "\n",
    "    return history_callback, pred_result, labels_val, pred_result_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-10T09:05:11.928Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:16: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 50, 300)           4500300   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 256)               570368    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                32832     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 8)                 520       \n",
      "=================================================================\n",
      "Total params: 5,235,604\n",
      "Trainable params: 5,235,604\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7eff0b32e730> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7eff0b32e730> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      " 768/1165 [==================>...........] - ETA: 6:25 - loss: 1.7908 - accuracy: 0.3541"
     ]
    }
   ],
   "source": [
    "history, pred_result, labels_val, pred_result_val  = model_setting(train_sequence, train_label, word2vec_matrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-10T09:05:29.967Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_result_val[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-10T09:05:30.183Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_result_val = label_decode(label_encoder, pred_result_val)\n",
    "pred_result_val[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-10T09:05:30.392Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('testing accuracy: {}'.format(round(accuracy_score(label_decode(label_encoder, labels_val), pred_result_val), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-10T09:05:30.619Z"
    }
   },
   "outputs": [],
   "source": [
    "training_log = pd.DataFrame()\n",
    "training_log = pd.read_csv(\"logs/training_log_LSTM.csv\")\n",
    "training_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-10T09:05:31.049Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Train accuracy', 'Val accuracy'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Train loss', 'Val loss'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-10T09:05:31.552Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_form = pd.read_pickle(\"test_df.pkl\")\n",
    "pred_form = pred_form.drop('hashtags', axis=1)\n",
    "pred_form = pred_form.drop('text', axis=1)\n",
    "pred_form = pred_form.drop('identification', axis=1)\n",
    "pred_form.rename(columns={'tweet_id':'id'}, inplace=True)\n",
    "\n",
    "pred_form['emotion'] = label_decode(label_encoder, pred_result)\n",
    "pred_form.to_csv('prediction_lstm.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other method I've tried"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# METHOD1： DECISION TREE\n",
    "the result is not good, the best accuracy = 0.32959"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# build analyzers (bag-of-words)\n",
    "BOW_500 = CountVectorizer(max_features=500, tokenizer=nltk.word_tokenize) \n",
    "\n",
    "# apply analyzer to training data\n",
    "tfidf_vectorizer.fit(train_df['text'])\n",
    "\n",
    "train_data_TFIDF_features_10000 = tfidf_vectorizer.transform(train_df['text'])\n",
    "\n",
    "## check dimension\n",
    "train_data_TFIDF_features_10000.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# for a classificaiton problem, you need to provide both training & testing data\n",
    "X_train = tfidf_vectorizer.transform(train_df['text'])\n",
    "y_train = train_df['emotion']\n",
    "\n",
    "X_test = tfidf_vectorizer.transform(test_df['text'])\n",
    "y_test = test_df['emotion']\n",
    "\n",
    "## take a look at data dimension is a good habbit  :)\n",
    "print('X_train.shape: ', X_train.shape)\n",
    "print('y_train.shape: ', y_train.shape)\n",
    "print('X_test.shape: ', X_test.shape)\n",
    "print('y_test.shape: ', y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## build DecisionTree model\n",
    "DT_model = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "## training!\n",
    "DT_model = DT_model.fit(X_train, y_train)\n",
    "\n",
    "## predict!\n",
    "y_train_pred = DT_model.predict(X_train)\n",
    "y_test_pred = DT_model.predict(X_test)\n",
    "\n",
    "## so we get the pred result\n",
    "y_test_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_form = pd.read_pickle(\"test_df.pkl\")\n",
    "pred_form = pred_form.drop('hashtags', axis=1)\n",
    "pred_form = pred_form.drop('text', axis=1)\n",
    "pred_form = pred_form.drop('identification', axis=1)\n",
    "pred_form.rename(columns={'tweet_id':'id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_form['emotion'] = y_test_pred\n",
    "pred_form.to_csv('prediction.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# METHOD2： DEEP LEARNING\n",
    "This model has accuracy 0.42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data (X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_pickle(\"train_cleaned_df.pkl\")\n",
    "test_df = pd.read_pickle(\"test_cleaned_df.pkl\")\n",
    "train_df = train_df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=15000, \n",
    "                             use_idf=True,  \n",
    "                             tokenizer=tweet_tokenizer.tokenize,\n",
    "                             stop_words=stopwords.words('english').append('lh')\n",
    "                             )\n",
    "\n",
    "tfidf_vectorizer.fit(train_df['cleaned'])data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sep = int(train_df.shape[0] * 0.8) # 20% for validation\n",
    "\n",
    "train_dataset_X = train_df[:sep]\n",
    "train_dataset_y = train_df.emotion[:sep]\n",
    "validation_dataset_X = train_df[sep:]\n",
    "validation_dataset_y = train_df.emotion[sep:]\n",
    "\n",
    "all_trainset_X = train_df\n",
    "all_trainset_y = train_df.emotion\n",
    "\n",
    "print(train_dataset_X.shape)\n",
    "print(train_dataset_y.shape)\n",
    "print(validation_dataset_X.shape)\n",
    "print(validation_dataset_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "# standardize name (X, y) \n",
    "X_train = tfidf_vectorizer.transform(train_dataset_X['cleaned'])\n",
    "y_train = train_dataset_y\n",
    "\n",
    "X_val = tfidf_vectorizer.transform(validation_dataset_X['cleaned'])\n",
    "y_val = validation_dataset_y\n",
    "\n",
    "all_X_train =  tfidf_vectorizer.transform(all_trainset_X['cleaned'])\n",
    "all_y_train = all_trainset_y\n",
    "\n",
    "## check dimension is a good habbit \n",
    "print('X_train.shape: ', X_train.shape)\n",
    "print('y_train.shape: ', y_train.shape)\n",
    "print('X_val.shape: ', X_val.shape)\n",
    "print('y_val.shape: ', y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_X = tfidf_vectorizer.transform(test_df['cleaned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('x_test_data.shape: ', test_dataset_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deal with categorical label (y)\n",
    "\n",
    "Rather than put your label `train_df['emotion']` directly into a model, we have to process these categorical (or say nominal) label by ourselves. \n",
    "\n",
    "Here, we use the basic method [one-hot encoding](https://en.wikipedia.org/wiki/One-hot) to transform our categorical  labels to numerical ones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## deal with label (string -> one-hot)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "print('check label: ', label_encoder.classes_)\n",
    "print('\\n## Before convert')\n",
    "print('y_train[0:7]:\\n', y_train[0:7])\n",
    "print('\\ny_train.shape: ', y_train.shape)\n",
    "print('y_val.shape: ', y_val.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encode(le, labels):\n",
    "    enc = le.transform(labels)\n",
    "    return keras.utils.to_categorical(enc)\n",
    "\n",
    "def label_decode(le, one_hot_label):\n",
    "    dec = np.argmax(one_hot_label, axis=1)\n",
    "    return le.inverse_transform(dec)\n",
    "\n",
    "y_train = label_encode(label_encoder, y_train)\n",
    "y_val = label_encode(label_encoder, y_val)\n",
    "all_y_train = label_encode(label_encoder, all_y_train)\n",
    "\n",
    "print('\\n\\n## After convert')\n",
    "print('y_train[0:7]:\\n', y_train[0:7])\n",
    "print('\\ny_train.shape: ', y_train.shape)\n",
    "print('y_val.shape: ', y_val.shape)\n",
    "print('all_y_train.shape: ', all_y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BUILD MODEL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I/O check\n",
    "input_shape = X_train.shape[1]\n",
    "print('input_shape: ', input_shape)\n",
    "\n",
    "output_shape = len(label_encoder.classes_)\n",
    "print('output_shape: ', output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, ReLU, Softmax, Dense\n",
    "\n",
    "# input layer\n",
    "model_input = Input(shape=(input_shape, ))  # 10000\n",
    "X = model_input\n",
    "\n",
    "# 1st hidden layer\n",
    "X_W1 = Dense(units=256)(X)  # 512\n",
    "H1 = ReLU()(X_W1)\n",
    "\n",
    "# 1st hidden layer\n",
    "H1_W2 = Dense(units=64)(H1)  # 256\n",
    "H2 = ReLU()(H1_W2)\n",
    "\n",
    "# 2nd hidden layer\n",
    "H2_W3 = Dense(units=32)(H2)  # 64\n",
    "H3 = ReLU()(H2_W3)\n",
    "\n",
    "\n",
    "# 2nd hidden layer\n",
    "H3_W4 = Dense(units=8)(H3)  # 32\n",
    "H4 = Softmax()(H3_W4)\n",
    "\n",
    "# output layer\n",
    "#H4_W5 = Dense(units=output_shape)(H4)  # 8\n",
    "#H5 = Softmax()(H4_W5)\n",
    "\n",
    "model_output = H4\n",
    "\n",
    "# create model\n",
    "model = Model(inputs=[model_input], outputs=[model_output])\n",
    "\n",
    "# loss function & optimizer\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# show model construction\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_logger = CSVLogger('logs/training_log_kaggle.csv')\n",
    "\n",
    "# training setting\n",
    "epochs = 5\n",
    "batch_size = 1000\n",
    "\n",
    "# training!\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size, \n",
    "                    callbacks=[csv_logger],\n",
    "                    validation_data = (X_val, y_val)\n",
    "                    )\n",
    "print('training finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## predict\n",
    "pred_result_val = model.predict(X_val, batch_size=1000)\n",
    "pred_result_val[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_result_val = label_decode(label_encoder, pred_result_val)\n",
    "pred_result_val[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('testing accuracy: {}'.format(round(accuracy_score(label_decode(label_encoder, y_val), pred_result_val), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's take a look at the training log\n",
    "training_log = pd.DataFrame()\n",
    "training_log = pd.read_csv(\"logs/training_log.csv\")\n",
    "training_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Train accuracy', 'Val accuracy'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Train loss', 'Val loss'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_form = pd.read_pickle(\"test_df.pkl\")\n",
    "pred_form = pred_form.drop('hashtags', axis=1)\n",
    "pred_form = pred_form.drop('text', axis=1)\n",
    "pred_form = pred_form.drop('identification', axis=1)\n",
    "pred_form.rename(columns={'tweet_id':'id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_dense = model.predict(test_dataset_X)\n",
    "y_test_dense = label_decode(label_encoder, y_test_dense)\n",
    "pred_form['emotion'] = y_test_dense\n",
    "pred_form.to_csv('prediction_dense.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
