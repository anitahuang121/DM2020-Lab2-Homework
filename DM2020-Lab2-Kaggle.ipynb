{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "\n",
    "Name: ÈªÉÊúâÁíø\n",
    "\n",
    "Student ID: 109062505\n",
    "\n",
    "GitHub ID: anitahuang121\n",
    "\n",
    "Kaggle name: Anita210503\n",
    "\n",
    "Kaggle private scoreboard snapshot: \n",
    "\n",
    "<img src=\"pics/KAGGLE.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T06:18:36.222684Z",
     "start_time": "2020-12-07T06:18:22.960690Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "emotion_data = pd.read_csv(\"emotion.csv\" , sep = \",\" , names = [\"tweet_id\" , \"emotion\"] , skiprows=[0])\n",
    "identification_data = pd.read_csv(\"data_identification.csv\" , sep = \",\" , names = [\"tweet_id\" , \"identification\"], skiprows=[0])\n",
    "tweets_data = pd.read_json(\"tweets_DM.json\" , lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T06:18:36.300489Z",
     "start_time": "2020-12-07T06:18:36.223791Z"
    }
   },
   "outputs": [],
   "source": [
    "identification_data.loc[identification_data['identification'] == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T06:18:36.308705Z",
     "start_time": "2020-12-07T06:18:36.301693Z"
    }
   },
   "outputs": [],
   "source": [
    "tweets_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T06:19:02.473414Z",
     "start_time": "2020-12-07T06:18:36.309717Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "data_df = json_normalize(tweets_data._source)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T06:19:02.909101Z",
     "start_time": "2020-12-07T06:19:02.474662Z"
    }
   },
   "outputs": [],
   "source": [
    "data_df = data_df.reindex(columns=['tweet.tweet_id','tweet.hashtags','tweet.text'])\n",
    "data_df = data_df.rename(columns={'tweet.tweet_id':'tweet_id' , 'tweet.hashtags':'hashtags' , 'tweet.text':'text' })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T06:19:02.925482Z",
     "start_time": "2020-12-07T06:19:02.910165Z"
    }
   },
   "outputs": [],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T06:19:04.572409Z",
     "start_time": "2020-12-07T06:19:02.926503Z"
    }
   },
   "outputs": [],
   "source": [
    "data_merge_emotion = pd.merge(data_df, emotion_data , on=['tweet_id'], how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T06:19:06.316740Z",
     "start_time": "2020-12-07T06:19:04.573531Z"
    }
   },
   "outputs": [],
   "source": [
    "data_all_info = pd.merge(data_merge_emotion, identification_data , on=['tweet_id'], how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T06:19:06.327241Z",
     "start_time": "2020-12-07T06:19:06.317807Z"
    }
   },
   "outputs": [],
   "source": [
    "data_all_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T06:19:06.758914Z",
     "start_time": "2020-12-07T06:19:06.328124Z"
    }
   },
   "outputs": [],
   "source": [
    "[(data_all_info['identification'] == 'train') & (data_all_info['emotion'].isnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T06:19:06.859100Z",
     "start_time": "2020-12-07T06:19:06.759986Z"
    }
   },
   "outputs": [],
   "source": [
    "data_all_info.loc[data_all_info['identification'] == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T06:19:07.052547Z",
     "start_time": "2020-12-07T06:19:06.860382Z"
    }
   },
   "outputs": [],
   "source": [
    "grouped = data_all_info.groupby(data_all_info.identification) \n",
    "train_df = grouped.get_group(\"train\") \n",
    "test_df = grouped.get_group(\"test\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T06:19:07.062400Z",
     "start_time": "2020-12-07T06:19:07.053759Z"
    }
   },
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T06:19:07.078251Z",
     "start_time": "2020-12-07T06:19:07.063372Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T06:19:09.345607Z",
     "start_time": "2020-12-07T06:19:07.079244Z"
    }
   },
   "outputs": [],
   "source": [
    "## save to pickle file\n",
    "\n",
    "train_df.to_pickle(\"train_df.pkl\") \n",
    "test_df.to_pickle(\"test_df.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "remove punctuation, noise such as '<LH>', numbers, stop_words\n",
    "    \n",
    "    and do stemming and lemmalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T08:21:51.954382Z",
     "start_time": "2020-12-09T08:21:48.300490Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import RegexpTokenizer\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T08:21:51.957741Z",
     "start_time": "2020-12-09T08:21:51.955686Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/hsnl-iot/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import RegexpTokenizer\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T08:21:55.889907Z",
     "start_time": "2020-12-09T08:21:54.048716Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## load a pickle file\n",
    "train_df = pd.read_pickle(\"train_df.pkl\")\n",
    "test_df = pd.read_pickle(\"test_df.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T15:54:04.330230Z",
     "start_time": "2020-12-08T15:53:53.463066Z"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "def remove_punctuation(text):\n",
    "    no_punct = \"\".join([c for c in text if c not in string.punctuation])\n",
    "    return no_punct\n",
    "\n",
    "train_df['text_without_punct'] = train_df['text'].apply(lambda x : remove_punctuation(x))\n",
    "test_df['text_without_punct'] = test_df['text'].apply(lambda x : remove_punctuation(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T15:55:00.771958Z",
     "start_time": "2020-12-08T15:54:45.349640Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "filt = '[!\"#$%&()*,-./:;<=>?@[\\]^_`{|}~ ]'\n",
    "\n",
    "train_df['split'] = train_df['text'] .apply(lambda x: x.replace(\"<LH>\",\"\"))\n",
    "train_df['split'] = train_df['split'].apply(lambda x: re.sub(filt, ' ', x.lower()))\n",
    "train_df['split'] = train_df['split'].apply(lambda x: re.sub(r'\\d+', \" \", x))\n",
    "train_df['split'] = train_df['split'].apply(lambda x: re.sub('<[^<]+?>', \" \", x))\n",
    "train_df['split'] = train_df['split'].apply(lambda x: x.split(\" \"))\n",
    "\n",
    "test_df['split'] = test_df['text'] .apply(lambda x: x.replace(\"<LH>\",\"\"))\n",
    "test_df['split'] = test_df['split'].apply(lambda x: re.sub(filt, ' ', x.lower()))\n",
    "test_df['split'] = test_df['split'].apply(lambda x: re.sub(r'\\d+', \" \", x))\n",
    "test_df['split'] = test_df['split'].apply(lambda x: re.sub('<[^<]+?>', \" \", x))\n",
    "test_df['split'] = test_df['split'].apply(lambda x: x.split(\" \"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T15:55:08.570888Z",
     "start_time": "2020-12-08T15:55:00.773224Z"
    }
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "train_df['no_stopwords'] = train_df['split'].apply(lambda x: [item for item in x if item not in stop_words])\n",
    "train_df['no_stopwords'] = train_df['no_stopwords'].apply(lambda x: [item for item in x if len(item) > 1])\n",
    "test_df['no_stopwords'] = test_df['split'].apply(lambda x: [item for item in x if item not in stop_words])\n",
    "test_df['no_stopwords'] = test_df['no_stopwords'].apply(lambda x: [item for item in x if len(item) > 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T15:59:20.231121Z",
     "start_time": "2020-12-08T15:55:08.571990Z"
    }
   },
   "outputs": [],
   "source": [
    "porter = nltk.stem.PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "train_df['stemmed'] = train_df['no_stopwords'].apply(lambda x: [porter.stem(y) for y in x])\n",
    "test_df['stemmed'] = test_df['no_stopwords'].apply(lambda x: [porter.stem(y) for y in x])\n",
    "train_df['lemmatization'] = train_df['no_stopwords'].apply(lambda x: [lemmatizer.lemmatize(y) for y in x])\n",
    "test_df['lemmatization'] = test_df['no_stopwords'].apply(lambda x: [lemmatizer.lemmatize(y) for y in x])\n",
    "train_df['lem_ste'] = train_df['lemmatization'].apply(lambda x: [lemmatizer.lemmatize(y) for y in x])\n",
    "test_df['lem_ste'] = test_df['lemmatization'].apply(lambda x: [lemmatizer.lemmatize(y) for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T15:59:23.736985Z",
     "start_time": "2020-12-08T15:59:20.232208Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df['clean'] = train_df['lem_ste'].apply(lambda x: [item for item in x if not item.isnumeric() ])\n",
    "test_df['clean'] = test_df['lem_ste'].apply(lambda x: [item for item in x if not item.isnumeric() ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T15:59:24.274901Z",
     "start_time": "2020-12-08T15:59:23.738111Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df['cleaned'] = train_df['clean'].apply(' '.join)\n",
    "test_df['cleaned'] = test_df['clean'].apply(' '.join)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T08:23:17.100628Z",
     "start_time": "2020-12-09T08:22:43.111005Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hsnl-iot/anaconda3/envs/tensorflow/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_features=15000,\n",
       "                tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7ff2c6230850>>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=15000, \n",
    "                             use_idf=True,  \n",
    "                             tokenizer=tweet_tokenizer.tokenize,\n",
    "                             stop_words=stopwords.words('english').append('lh')\n",
    "                             )\n",
    "\n",
    "tfidf_vectorizer.fit(train_df['cleaned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T16:02:28.647759Z",
     "start_time": "2020-12-08T16:02:28.632516Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'\", '+', '\\\\', 'a', \"a's\", 'aa', 'aaa', 'aajtak', 'aamir', 'aap', 'aaron', 'aaroncarter', 'aaronrodgers', 'aarti', 'ab', 'abandon', 'abandoned', 'abba', 'abbey', 'abby', 'abc', 'abcnetwork', 'abcnews', 'abcpolitics', 'aberdeen', 'abhlatino', 'abi', 'abide', 'abiding', 'ability', 'able', 'aboard', 'abominable', 'abomination', 'abortion', 'abound', 'about', 'aboveonly', 'abraham', 'abroad', 'absence', 'absent', 'absolute', 'absolutely', 'absorbed', 'absurd', 'abt', 'abu', 'abundance', 'abundant', 'abundantly', 'abuse', 'abused', 'abuser', 'abusing', 'abusive', 'abyss', 'ac', 'aca', 'academic', 'academy', 'acc', 'accent', 'accept', 'acceptable', 'acceptance', 'accepted', 'accepting', 'accepts', 'access', 'accessible', 'accessory', 'accident', 'accidentally', 'accomplish', 'accomplished', 'accomplishing', 'accomplishment', 'accord', 'according', 'accordingly', 'account', 'accountability', 'accountable', 'accountant', 'accounting', 'acct', 'accuracy', 'accurate', 'accusation', 'accuse', 'accused', 'accusing', 'ace', 'ache', 'achieve', 'achieved', 'achievement', 'achievemints', 'achieving']\n",
      "['üôÄ', 'üôÅ', 'üôÇ', 'üôÉ', 'üôÑ', 'üôÖ', 'üôÜ', 'üôá', 'üôà', 'üôâ', 'üôä', 'üôã', 'üôå', 'üôç', 'üôè', 'üöÄ', 'üöÇ', 'üöë', 'üöí', 'üöî', 'üöó', 'üöò', 'üöô', 'üö®', 'üö´', 'üö¨', 'üöÆ', 'üö¥', 'üö∂', 'üõå', 'üõç', 'üõê', 'üõ´', 'ü§ê', 'ü§ë', 'ü§í', 'ü§ì', 'ü§î', 'ü§ï', 'ü§ñ', 'ü§ó', 'ü§ò', 'ü§ô', 'ü§ö', 'ü§õ', 'ü§ú', 'ü§ù', 'ü§û', 'ü§ü', 'ü§†', 'ü§°', 'ü§¢', 'ü§£', 'ü§§', 'ü§•', 'ü§¶', 'ü§ß', 'ü§®', 'ü§©', 'ü§™', 'ü§´', 'ü§¨', 'ü§≠', 'ü§Æ', 'ü§Ø', 'ü§∞', 'ü§≥', 'ü§¥', 'ü§µ', 'ü§∂', 'ü§∑', 'ü§∏', 'ü•Ä', 'ü•Å', 'ü•Ç', 'ü•É', 'ü•á', 'ü•ä', 'ü•ë', 'ü•ì', 'ü•ï', 'ü•ó', 'ü•û', 'ü•ß', 'ü¶Å', 'ü¶É', 'ü¶Ñ', 'ü¶Ö', 'ü¶á', 'ü¶à', 'ü¶â', 'ü¶ã', 'ü¶å', 'üßÄ', 'üßê', 'üß°', '\\U000e0062', '\\U000e0067', '\\U000e0073', '\\U000e007f']\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_vectorizer.get_feature_names()[:100])\n",
    "print(tfidf_vectorizer.get_feature_names()[-100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T16:02:52.381019Z",
     "start_time": "2020-12-08T16:02:30.564976Z"
    }
   },
   "outputs": [],
   "source": [
    "#save cleaned data\n",
    "train_df.to_pickle(\"train_cleaned_df.pkl\") \n",
    "test_df.to_pickle(\"test_cleaned_df.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T08:23:46.589195Z",
     "start_time": "2020-12-09T08:23:25.295505Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_pickle(\"train_cleaned_df.pkl\")\n",
    "test_df = pd.read_pickle(\"test_cleaned_df.pkl\")\n",
    "train_df = train_df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T08:23:17.100628Z",
     "start_time": "2020-12-09T08:22:43.111005Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hsnl-iot/anaconda3/envs/tensorflow/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(max_features=15000,\n",
       "                tokenizer=<bound method TweetTokenizer.tokenize of <nltk.tokenize.casual.TweetTokenizer object at 0x7ff2c6230850>>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=15000, \n",
    "                             use_idf=True,  \n",
    "                             tokenizer=tweet_tokenizer.tokenize,\n",
    "                             stop_words=stopwords.words('english').append('lh')\n",
    "                             )\n",
    "\n",
    "tfidf_vectorizer.fit(train_df['cleaned'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final method that I used: Embedding + LSTM + Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EMBEDDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T11:49:21.184954Z",
     "start_time": "2020-12-08T11:49:21.171290Z"
    }
   },
   "outputs": [],
   "source": [
    "Dictionary = {}\n",
    "for idx, token in enumerate(tfidf_vectorizer.get_feature_names()):\n",
    "    Dictionary[token] = idx\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T09:00:58.122170Z",
     "start_time": "2020-12-08T08:59:17.848917Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model_path = \"GoogleNews/GoogleNews-vectors-negative300.bin.gz\"\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(model_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T11:55:34.158963Z",
     "start_time": "2020-12-08T11:55:34.153776Z"
    }
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "def word2vec_matrix():\n",
    "    matrix_emb = np.zeros((len(Dictionary)+1, EMBEDDING_DIM)) # replace MAX_WORD to len(Dictionary)\n",
    "    for word, idx in Dictionary.items():\n",
    "        try:\n",
    "            vector =  word2vec_model.wv[word]\n",
    "        except:\n",
    "            vector = np.zeros(300,)\n",
    "        if idx < 15000:\n",
    "            matrix_emb[idx] = np.array(vector)\n",
    "    matrix_emb[15000] = np.zeros(300,)     \n",
    "    return matrix_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T11:55:40.505251Z",
     "start_time": "2020-12-08T11:55:40.499672Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 50\n",
    "\n",
    "def text_to_sequence(texts):\n",
    "    tokenized_texts = []\n",
    "    for string in texts:\n",
    "        tokenized_text = []\n",
    "        for idx, word in enumerate(string.split(' ')):\n",
    "            \n",
    "            # truncate\n",
    "            if idx >= MAX_SEQUENCE_LENGTH: break \n",
    "                \n",
    "            try:\n",
    "                token = token_dict[word.lower()]\n",
    "            except:\n",
    "                token = 10000\n",
    "            tokenized_text.append(token)\n",
    "            \n",
    "        # padding \n",
    "        if len(tokenized_text) < MAX_SEQUENCE_LENGTH: \n",
    "            tokenized_text.extend([10000]*(MAX_SEQUENCE_LENGTH-len(tokenized_text)))\n",
    "                \n",
    "        tokenized_texts.append(tokenized_text)\n",
    "\n",
    "    return np.array(tokenized_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T11:55:40.727690Z",
     "start_time": "2020-12-08T11:55:40.719392Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "emotion_array = ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'sadness', 'surprise','trust']\n",
    "def emotion_to_sequence(emotions):\n",
    "    tokenized_emotions = []\n",
    "    for string in emotions:\n",
    "        token = emotion_array.index(string)\n",
    "        tokenized_emotions.append(token)\n",
    "    return to_categorical(tokenized_emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T11:55:41.063461Z",
     "start_time": "2020-12-08T11:55:41.057638Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check label:  ['anger' 'anticipation' 'disgust' 'fear' 'joy' 'sadness' 'surprise'\n",
      " 'trust']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(emotion_array)\n",
    "\n",
    "print('check label: ', label_encoder.classes_)\n",
    "def label_encode(le, labels):\n",
    "    enc = le.transform(labels)\n",
    "    return keras.utils.to_categorical(enc)\n",
    "def label_decode(le, one_hot_label):\n",
    "    dec = np.argmax(one_hot_label, axis=1)\n",
    "    return le.inverse_transform(dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T11:55:41.160913Z",
     "start_time": "2020-12-08T11:55:41.155031Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout, Embedding, GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T11:55:43.773389Z",
     "start_time": "2020-12-08T11:55:41.402975Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = train_df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T11:55:58.296424Z",
     "start_time": "2020-12-08T11:55:43.774586Z"
    }
   },
   "outputs": [],
   "source": [
    "train_sequence = text_to_sequence(train_df['cleaned'])\n",
    "train_label = emotion_to_sequence(train_df['emotion'])\n",
    "\n",
    "test_sequence = text_to_sequence(test_df['cleaned'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T14:55:46.748202Z",
     "start_time": "2020-12-08T14:55:46.741696Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_WORD = 15000\n",
    "LSTM1_DIM = 256\n",
    "DENSE1_DIM = 256\n",
    "DENSE2_DIM = 256\n",
    "CATEGORY_NUM = 8\n",
    "BATCH_SIZE = 1000\n",
    "VALIDATION_SPLIT = 0.2\n",
    "EPOCHS = 6\n",
    "\n",
    "csv_logger = CSVLogger('logs/training_log_LSTM.csv')\n",
    "                    \n",
    "\n",
    "def model_setting(texts, labels, matrix_emb=\"uniform\"):    \n",
    "    DROPOUT = 0.2\n",
    "    \n",
    "    if matrix_emb != \"uniform\": matrix_emb = Constant(matrix_emb)\n",
    "    \n",
    "#     split data to train & validation\n",
    "    texts_train, texts_val, labels_train, labels_val = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # model build\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(MAX_WORD+1, EMBEDDING_DIM, embeddings_initializer=matrix_emb, input_length=MAX_SEQUENCE_LENGTH, trainable=True)) # with embedding matrix\n",
    "    model.add(LSTM(LSTM1_DIM, dropout=DROPOUT, recurrent_dropout=DROPOUT))\n",
    "    model.add(Dense(DENSE1_DIM, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(DENSE2_DIM, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(CATEGORY_NUM, kernel_initializer='normal', activation='softmax'))\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "    model.summary()\n",
    "\n",
    "    \n",
    "    history_callback = model.fit(\n",
    "        texts_train, \n",
    "        labels_train, \n",
    "        epochs=EPOCHS, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        callbacks=[csv_logger],\n",
    "        verbose=1, \n",
    "        shuffle=True, \n",
    "        validation_data=(texts_val, labels_val))\n",
    "    \n",
    "    pred_result = model.predict(test_sequence, batch_size=1000)\n",
    "    \n",
    "    pred_result_val = model.predict(texts_val, batch_size=1000)\n",
    "\n",
    "    return history_callback, pred_result, labels_val, pred_result_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-08T14:55:46.716Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-87177b11f884>:6: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  vector =  word2vec_model.wv[word]\n",
      "<ipython-input-20-1456ad931e06>:16: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if matrix_emb != \"uniform\": matrix_emb = Constant(matrix_emb)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 50, 300)           4500300   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 256)               570368    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 8)                 2056      \n",
      "=================================================================\n",
      "Total params: 5,204,308\n",
      "Trainable params: 5,204,308\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "  19/1165 [..............................] - ETA: 11:14 - loss: 2.0772 - accuracy: 0.1770"
     ]
    }
   ],
   "source": [
    "history, pred_result, labels_val, pred_result_val  = model_setting(train_sequence, train_label, word2vec_matrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-08T14:55:46.888Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_result_val[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-08T14:55:47.046Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_result_val = label_decode(label_encoder, pred_result_val)\n",
    "pred_result_val[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-08T14:55:47.246Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('testing accuracy: {}'.format(round(accuracy_score(label_decode(label_encoder, labels_val), pred_result_val), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-08T14:55:47.736Z"
    }
   },
   "outputs": [],
   "source": [
    "training_log = pd.DataFrame()\n",
    "training_log = pd.read_csv(\"logs/training_log_LSTM.csv\")\n",
    "training_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-08T14:55:48.216Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Train accuracy', 'Val accuracy'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Train loss', 'Val loss'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-08T14:55:49.045Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_form = pd.read_pickle(\"test_df.pkl\")\n",
    "pred_form = pred_form.drop('hashtags', axis=1)\n",
    "pred_form = pred_form.drop('text', axis=1)\n",
    "pred_form = pred_form.drop('identification', axis=1)\n",
    "pred_form.rename(columns={'tweet_id':'id'}, inplace=True)\n",
    "\n",
    "pred_form['emotion'] = label_decode(label_encoder, pred_result)\n",
    "pred_form.to_csv('prediction_lstm.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "labels_val_decode = label_decode(label_encoder, labels_val)\n",
    "pred_result_val_decode = label_decode(label_encoder, pred_result_val)\n",
    "print(classification_report(y_true=labels_val_decode, y_pred=pred_result_val_decode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other method I have tried"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# METHOD1 : DECISION TREE\n",
    "the result is not good, the best accuracy = 0.32959"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T07:58:18.901022Z",
     "start_time": "2020-12-04T07:56:33.211618Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# build analyzers (bag-of-words)\n",
    "BOW_500 = CountVectorizer(max_features=500, tokenizer=nltk.word_tokenize) \n",
    "\n",
    "# apply analyzer to training data\n",
    "tfidf_vectorizer.fit(train_df['text'])\n",
    "\n",
    "train_data_TFIDF_features_10000 = tfidf_vectorizer.transform(train_df['text'])\n",
    "\n",
    "## check dimension\n",
    "train_data_TFIDF_features_10000.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T07:59:28.478773Z",
     "start_time": "2020-12-04T07:58:18.901966Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# for a classificaiton problem, you need to provide both training & testing data\n",
    "X_train = tfidf_vectorizer.transform(train_df['text'])\n",
    "y_train = train_df['emotion']\n",
    "\n",
    "X_test = tfidf_vectorizer.transform(test_df['text'])\n",
    "y_test = test_df['emotion']\n",
    "\n",
    "## take a look at data dimension is a good habbit  :)\n",
    "print('X_train.shape: ', X_train.shape)\n",
    "print('y_train.shape: ', y_train.shape)\n",
    "print('X_test.shape: ', X_test.shape)\n",
    "print('y_test.shape: ', y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T09:25:10.078659Z",
     "start_time": "2020-12-04T08:05:59.730051Z"
    }
   },
   "outputs": [],
   "source": [
    "## build DecisionTree model\n",
    "DT_model = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "## training!\n",
    "DT_model = DT_model.fit(X_train, y_train)\n",
    "\n",
    "## predict!\n",
    "y_train_pred = DT_model.predict(X_train)\n",
    "y_test_pred = DT_model.predict(X_test)\n",
    "\n",
    "## so we get the pred result\n",
    "y_test_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T09:30:16.625944Z",
     "start_time": "2020-12-04T09:30:16.163748Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_form = pd.read_pickle(\"test_df.pkl\")\n",
    "pred_form = pred_form.drop('hashtags', axis=1)\n",
    "pred_form = pred_form.drop('text', axis=1)\n",
    "pred_form = pred_form.drop('identification', axis=1)\n",
    "pred_form.rename(columns={'tweet_id':'id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-04T09:30:18.168399Z",
     "start_time": "2020-12-04T09:30:17.855238Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_form['emotion'] = y_test_pred\n",
    "pred_form.to_csv('prediction.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# METHOD2 : DEEP LEARNING\n",
    "\n",
    "best test accuracy = 0.42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T08:23:46.589195Z",
     "start_time": "2020-12-09T08:23:25.295505Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_pickle(\"train_cleaned_df.pkl\")\n",
    "test_df = pd.read_pickle(\"test_cleaned_df.pkl\")\n",
    "train_df = train_df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data (X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T08:23:52.110015Z",
     "start_time": "2020-12-09T08:23:46.590465Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1164450, 13)\n",
      "(1164450,)\n",
      "(291113, 13)\n",
      "(291113,)\n"
     ]
    }
   ],
   "source": [
    "sep = int(train_df.shape[0] * 0.8) # 20% for validation\n",
    "\n",
    "train_dataset_X = train_df[:sep]\n",
    "train_dataset_y = train_df.emotion[:sep]\n",
    "validation_dataset_X = train_df[sep:]\n",
    "validation_dataset_y = train_df.emotion[sep:]\n",
    "\n",
    "all_trainset_X = train_df\n",
    "all_trainset_y = train_df.emotion\n",
    "\n",
    "print(train_dataset_X.shape)\n",
    "print(train_dataset_y.shape)\n",
    "print(validation_dataset_X.shape)\n",
    "print(validation_dataset_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T08:24:58.679496Z",
     "start_time": "2020-12-09T08:23:52.111039Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape:  (1164450, 15000)\n",
      "y_train.shape:  (1164450,)\n",
      "X_val.shape:  (291113, 15000)\n",
      "y_val.shape:  (291113,)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras\n",
    "\n",
    "# standardize name (X, y) \n",
    "X_train = tfidf_vectorizer.transform(train_dataset_X['cleaned'])\n",
    "y_train = train_dataset_y\n",
    "\n",
    "X_val = tfidf_vectorizer.transform(validation_dataset_X['cleaned'])\n",
    "y_val = validation_dataset_y\n",
    "\n",
    "all_X_train =  tfidf_vectorizer.transform(all_trainset_X['cleaned'])\n",
    "all_y_train = all_trainset_y\n",
    "\n",
    "## check dimension is a good habbit \n",
    "print('X_train.shape: ', X_train.shape)\n",
    "print('y_train.shape: ', y_train.shape)\n",
    "print('X_val.shape: ', X_val.shape)\n",
    "print('y_val.shape: ', y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T08:25:09.132985Z",
     "start_time": "2020-12-09T08:24:58.680890Z"
    }
   },
   "outputs": [],
   "source": [
    "test_dataset_X = tfidf_vectorizer.transform(test_df['cleaned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T08:25:09.136020Z",
     "start_time": "2020-12-09T08:25:09.134166Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test_data.shape:  (411972, 15000)\n"
     ]
    }
   ],
   "source": [
    "print('x_test_data.shape: ', test_dataset_X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deal with categorical label (y)\n",
    "\n",
    "Rather than put your label `train_df['emotion']` directly into a model, we have to process these categorical (or say nominal) label by ourselves. \n",
    "\n",
    "Here, we use the basic method [one-hot encoding](https://en.wikipedia.org/wiki/One-hot) to transform our categorical  labels to numerical ones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T08:25:09.173618Z",
     "start_time": "2020-12-09T08:25:09.137115Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check label:  ['anger' 'anticipation' 'disgust' 'fear' 'joy' 'sadness' 'surprise'\n",
      " 'trust']\n",
      "\n",
      "## Before convert\n",
      "y_train[0:7]:\n",
      " 270465              joy\n",
      "1413126    anticipation\n",
      "1353851             joy\n",
      "1587836    anticipation\n",
      "389451              joy\n",
      "1243511    anticipation\n",
      "80836           sadness\n",
      "Name: emotion, dtype: object\n",
      "\n",
      "y_train.shape:  (1164450,)\n",
      "y_val.shape:  (291113,)\n"
     ]
    }
   ],
   "source": [
    "## deal with label (string -> one-hot)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "print('check label: ', label_encoder.classes_)\n",
    "print('\\n## Before convert')\n",
    "print('y_train[0:7]:\\n', y_train[0:7])\n",
    "print('\\ny_train.shape: ', y_train.shape)\n",
    "print('y_val.shape: ', y_val.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T08:25:09.474484Z",
     "start_time": "2020-12-09T08:25:09.174685Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "## After convert\n",
      "y_train[0:7]:\n",
      " [[0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]]\n",
      "\n",
      "y_train.shape:  (1164450, 8)\n",
      "y_val.shape:  (291113, 8)\n",
      "all_y_train.shape:  (1455563, 8)\n"
     ]
    }
   ],
   "source": [
    "def label_encode(le, labels):\n",
    "    enc = le.transform(labels)\n",
    "    return tensorflow.keras.utils.to_categorical(enc)\n",
    "\n",
    "def label_decode(le, one_hot_label):\n",
    "    dec = np.argmax(one_hot_label, axis=1)\n",
    "    return le.inverse_transform(dec)\n",
    "\n",
    "y_train = label_encode(label_encoder, y_train)\n",
    "y_val = label_encode(label_encoder, y_val)\n",
    "all_y_train = label_encode(label_encoder, all_y_train)\n",
    "\n",
    "print('\\n\\n## After convert')\n",
    "print('y_train[0:7]:\\n', y_train[0:7])\n",
    "print('\\ny_train.shape: ', y_train.shape)\n",
    "print('y_val.shape: ', y_val.shape)\n",
    "print('all_y_train.shape: ', all_y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BUILD MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T08:25:09.477708Z",
     "start_time": "2020-12-09T08:25:09.475467Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape:  15000\n",
      "output_shape:  8\n"
     ]
    }
   ],
   "source": [
    "# I/O check\n",
    "input_shape = X_train.shape[1]\n",
    "print('input_shape: ', input_shape)\n",
    "\n",
    "output_shape = len(label_encoder.classes_)\n",
    "print('output_shape: ', output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T08:25:09.532167Z",
     "start_time": "2020-12-09T08:25:09.478815Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 15000)]           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               3840256   \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 264       \n",
      "_________________________________________________________________\n",
      "softmax (Softmax)            (None, 8)                 0         \n",
      "=================================================================\n",
      "Total params: 3,859,048\n",
      "Trainable params: 3,859,048\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.layers import ReLU, Softmax\n",
    "\n",
    "# input layer\n",
    "model_input = Input(shape=(input_shape, ))  # 10000\n",
    "X = model_input\n",
    "\n",
    "# 1st hidden layer\n",
    "X_W1 = Dense(units=256)(X)  # 512\n",
    "H1 = ReLU()(X_W1)\n",
    "\n",
    "# 1st hidden layer\n",
    "H1_W2 = Dense(units=64)(H1)  # 256\n",
    "H2 = ReLU()(H1_W2)\n",
    "\n",
    "# 2nd hidden layer\n",
    "H2_W3 = Dense(units=32)(H2)  # 64\n",
    "H3 = ReLU()(H2_W3)\n",
    "\n",
    "\n",
    "# 2nd hidden layer\n",
    "H3_W4 = Dense(units=8)(H3)  # 32\n",
    "H4 = Softmax()(H3_W4)\n",
    "\n",
    "# output layer\n",
    "#H4_W5 = Dense(units=output_shape)(H4)  # 8\n",
    "#H5 = Softmax()(H4_W5)\n",
    "\n",
    "model_output = H4\n",
    "\n",
    "# create model\n",
    "model = Model(inputs=[model_input], outputs=[model_output])\n",
    "\n",
    "# loss function & optimizer\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# show model construction\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T08:25:09.534447Z",
     "start_time": "2020-12-09T08:24:16.235Z"
    }
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "indices[1] = [0,14760] is out of order. Many sparse ops require sorted indices.\n    Use `tf.sparse.reorder` to create a correctly ordered copy.\n\n [Op:SerializeManySparse]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-2d7ee1021884>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# training!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m history = model.fit(X_train, y_train, \n\u001b[0m\u001b[1;32m     11\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1048\u001b[0m          \u001b[0mtraining_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRespectCompiledTrainableState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m       \u001b[0;31m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m       data_handler = data_adapter.DataHandler(\n\u001b[0m\u001b[1;32m   1051\u001b[0m           \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m           \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m     \u001b[0madapter_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m     self._adapter = adapter_cls(\n\u001b[0m\u001b[1;32m   1101\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpack_x_y_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDatasetV2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m     \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensor_slices\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    689\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m     \"\"\"\n\u001b[0;32m--> 691\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTensorSliceDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m   \u001b[0;32mclass\u001b[0m \u001b[0m_GeneratorState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, element)\u001b[0m\n\u001b[1;32m   3155\u001b[0m     \u001b[0melement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3156\u001b[0m     \u001b[0mbatched_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_spec_from_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3157\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_batched_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatched_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3158\u001b[0m     self._structure = nest.map_structure(\n\u001b[1;32m   3159\u001b[0m         lambda component_spec: component_spec._unbatch(), batched_spec)  # pylint: disable=protected-access\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36mto_batched_tensor_list\u001b[0;34m(element_spec, element)\u001b[0m\n\u001b[1;32m    362\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m   \u001b[0;31m# pylint: disable=g-long-lambda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m   return _to_tensor_list_helper(\n\u001b[0m\u001b[1;32m    365\u001b[0m       lambda state, spec, component: state + spec._to_batched_tensor_list(\n\u001b[1;32m    366\u001b[0m           component), element_spec, element)\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36m_to_tensor_list_helper\u001b[0;34m(encode_fn, element_spec, element)\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mencode_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomponent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m   return functools.reduce(\n\u001b[0m\u001b[1;32m    340\u001b[0m       reduce_fn, zip(nest.flatten(element_spec), nest.flatten(element)), [])\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36mreduce_fn\u001b[0;34m(state, value)\u001b[0m\n\u001b[1;32m    335\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreduce_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomponent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mencode_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomponent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m   return functools.reduce(\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(state, spec, component)\u001b[0m\n\u001b[1;32m    363\u001b[0m   \u001b[0;31m# pylint: disable=g-long-lambda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m   return _to_tensor_list_helper(\n\u001b[0;32m--> 365\u001b[0;31m       lambda state, spec, component: state + spec._to_batched_tensor_list(\n\u001b[0m\u001b[1;32m    366\u001b[0m           component), element_spec, element)\n\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/framework/sparse_tensor.py\u001b[0m in \u001b[0;36m_to_batched_tensor_list\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    367\u001b[0m       raise ValueError(\n\u001b[1;32m    368\u001b[0m           \"Unbatching a sparse tensor is only supported for rank >= 1\")\n\u001b[0;32m--> 369\u001b[0;31m     return [gen_sparse_ops.serialize_many_sparse(\n\u001b[0m\u001b[1;32m    370\u001b[0m         \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m         out_type=dtypes.variant)]\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/ops/gen_sparse_ops.py\u001b[0m in \u001b[0;36mserialize_many_sparse\u001b[0;34m(sparse_indices, sparse_values, sparse_shape, out_type, name)\u001b[0m\n\u001b[1;32m    494\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6860\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6861\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6862\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6863\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.8/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: indices[1] = [0,14760] is out of order. Many sparse ops require sorted indices.\n    Use `tf.sparse.reorder` to create a correctly ordered copy.\n\n [Op:SerializeManySparse]"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "\n",
    "csv_logger = CSVLogger('logs/training_log_DENSE.csv')\n",
    "\n",
    "# training setting\n",
    "epochs = 5\n",
    "batch_size = 1000\n",
    "\n",
    "# training!\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size, \n",
    "                    callbacks=[csv_logger],\n",
    "                    validation_data = (X_val, y_val))\n",
    "print('training finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T08:25:09.535107Z",
     "start_time": "2020-12-09T08:24:16.462Z"
    }
   },
   "outputs": [],
   "source": [
    "## predict\n",
    "pred_result_val = model.predict(X_val, batch_size=1000)\n",
    "pred_result_val[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T08:25:09.535629Z",
     "start_time": "2020-12-09T08:24:16.692Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_result_val = label_decode(label_encoder, pred_result_val)\n",
    "pred_result_val[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T08:25:09.536269Z",
     "start_time": "2020-12-09T08:24:16.953Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('testing accuracy: {}'.format(round(accuracy_score(label_decode(label_encoder, y_val), pred_result_val), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T08:25:09.536838Z",
     "start_time": "2020-12-09T08:24:17.187Z"
    }
   },
   "outputs": [],
   "source": [
    "#Let's take a look at the training log\n",
    "training_log = pd.DataFrame()\n",
    "training_log = pd.read_csv(\"logs/training_log.csv\")\n",
    "training_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T08:25:09.537403Z",
     "start_time": "2020-12-09T08:24:17.420Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Train accuracy', 'Val accuracy'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Train loss', 'Val loss'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T08:25:09.538039Z",
     "start_time": "2020-12-09T08:24:17.640Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_form = pd.read_pickle(\"test_df.pkl\")\n",
    "pred_form = pred_form.drop('hashtags', axis=1)\n",
    "pred_form = pred_form.drop('text', axis=1)\n",
    "pred_form = pred_form.drop('identification', axis=1)\n",
    "pred_form.rename(columns={'tweet_id':'id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T08:25:09.538640Z",
     "start_time": "2020-12-09T08:24:18.138Z"
    }
   },
   "outputs": [],
   "source": [
    "y_test_dense = model.predict(test_dataset_X)\n",
    "y_test_dense = label_decode(label_encoder, y_test_dense)\n",
    "pred_form['emotion'] = y_test_dense\n",
    "pred_form.to_csv('prediction_dense.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 with tensorflow2.4.0rc1",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
